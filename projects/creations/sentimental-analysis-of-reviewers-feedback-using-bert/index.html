<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <title>Tan Zhou | Sentimental analysis of reviewers&#39; feedback using BERT vs. Machien learning</title>
  <meta property="og:title" content="Tan Zhou | Sentimental analysis of reviewers&#39; feedback using BERT vs. Machien learning" />
  <meta property="og:image" content="/img/tan_zhou.jpg" />
  <meta name="description" content="To predict sentiment (postive, neutral, negeative) of customer feedback using tweet texts of differnt airline companies and compare different models&#39;performace on text classification.">
  <meta property="og:description" content="To predict sentiment (postive, neutral, negeative) of customer feedback using tweet texts of differnt airline companies and compare different models&#39;performace on text classification." />
  <meta name="author" content="Tan Zhou">
  
  <link rel="preload" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.0.0/css/bootstrap.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
    <noscript><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.0.0/css/bootstrap.min.css"></noscript>
  
  <link rel="preload" href="https://fonts.googleapis.com/css?family=Saira+Extra+Condensed:100,200,300,400,500,600,700,800,900&display=swap" as="style" onload="this.onload=null;this.rel='stylesheet'">
    <noscript><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Saira+Extra+Condensed:100,200,300,400,500,600,700,800,900&display=swap"></noscript>
  <link rel="preload" href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,600,600i,700,700i,800,800i&display=swap" as="style" onload="this.onload=null;this.rel='stylesheet'">
      <noscript><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,600,600i,700,700i,800,800i&display=swap"></noscript>
  <link rel="preload" href="https://use.fontawesome.com/releases/v5.12.1/css/all.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
    <noscript><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.12.1/css/all.css"></noscript>
  <link rel="preload" href="https://cdnjs.cloudflare.com/ajax/libs/devicons/1.8.0/css/devicons.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
    <noscript><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/devicons/1.8.0/css/devicons.min.css"></noscript>
  <link rel="preload" href="https://cdnjs.cloudflare.com/ajax/libs/simple-line-icons/2.4.1/css/simple-line-icons.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
    <noscript><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/simple-line-icons/2.4.1/css/simple-line-icons.min.css"></noscript>
  
  <link rel="preload" href="/css/resume.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
    <noscript><link rel="stylesheet" href="/css/resume.css"></noscript>
  <link rel="preload" href="/css/tweaks.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
    <noscript><link rel="stylesheet" href="/css/tweaks.css"></noscript>
  <link rel="preload" href="/css/resume-override.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
    <noscript><link rel="stylesheet" href="/css/resume-override.css"></noscript>
  <meta name="generator" content="Hugo 0.64.1" />
  
   
  
</head>
<body id="page-top">
  <nav class="navbar navbar-expand-lg navbar-dark bg-primary fixed-top" id="sideNav">
  <a class="navbar-brand js-scroll-trigger" href="#page-top">
    <span class="d-block d-lg-none">Tan Zhou</span>
    <span class="d-none d-lg-block">
      <img class="img-fluid img-profile rounded-circle mx-auto mb-2" src="/img/tan_zhou.jpg" alt="">
    </span>
  </a>
  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>
  <div class="collapse navbar-collapse" id="navbarSupportedContent">
    <ul class="navbar-nav">
      <li class="nav-item">
        <a class="nav-link js-scroll-trigger" href="/#about">About</a>
      </li>
      
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="/#skills">Skills</a>
          </li>
      
      
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="/#projects">Projects</a>
          </li>
      
      
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="/#open">Open Source</a>
          </li>
      
      
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="/#Publications">Publications</a>
          </li>
      
      
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="/#experience">Experience</a>
          </li>
      
      
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="/#education">Education</a>
          </li>
      
      
    </ul>
  </div>
</nav>

  <div class="container-fluid p-0">
    
<nav aria-label="breadcrumb">
  <ol  class="breadcrumb">
    







<li class="breadcrumb-item">
  <a href="/">Home</a>
</li>


<li class="breadcrumb-item">
  <a href="/projects/">Projects</a>
</li>


<li class="breadcrumb-item">
  <a href="/projects/creations/">Creations</a>
</li>


<li class="breadcrumb-item active">
  <a href="/projects/creations/sentimental-analysis-of-reviewers-feedback-using-bert/">Sentimental analysis of reviewers&#39; feedback using BERT vs. Machien learning</a>
</li>

  </ol>
</nav>




<section class="resume-section p-3 p-lg-5 d-flex d-column">
  <div class="my-auto">
    <h2 class="mb-0"><span class="text-primary">Sentimental analysis of reviewers&#39; feedback using BERT vs. Machien learning</span></h2>
    <img src="/img/word_cloud_tweet.png" style="max-height:300px;max-width:30%;" align="right"/>
    <h1 id="introduction">Introduction</h1>
<p><strong>Note:</strong> Retraining the BERT model took a long time using a local computer, to run in the Google Colab will be a good choice</p>
<h2 id="objectives">Objectives</h2>
<p>To predict sentiment (postive, neutral, negeative) of customer feedback using tweet texts of differnt airline companies and compare different models&rsquo;performace on text classification.</p>
<p>Specifically, multiple machine learing models such as KNN, Random forest and SVC have been used for conduct classification as a baseline. A new language representation model named BERT (Bidirectional Encoder Representations from Transformers) was also implemented to conduct sentiment analysis.</p>
<h2 id="bert">BERT</h2>
<p>Bidirectional Encoder Representations from Transformers (BERT) is a technique for NLP (Natural Language Processing) pre-training developed by Google and published in 2018.</p>
<p>BERT is a method of pretraining language representations that was used to create models that NLP practicioners can then download and use for free. You can either use these models to extract high quality language features from your text data, or you can fine-tune these models on a specific task (classification, entity recognition, question answering, etc.) with your own data to produce state of the art predictions.</p>
<h3 id="how-bert-works">How BERT works?</h3>
<p>BERT was built on the Transformer, an attention mechanism that learns contextual relations between words (or sub-words) in a text. The attention mechanism was used to extratct information of context of a given words and then encode it in a learned vector. Generally, there are two mechanisms - an encoder that reads the text input and a decoder that produces a prediction for the task.</p>
<p>The detailed workings of Transformer are described in a <a href="https://arxiv.org/abs/1810.04805">paper by Google</a>. The figure below describe the brief steps of BERT during the traning process.</p>
<p><img src="./img/bert_embedding.png" alt=""></p>
<p>where the model takes a pair of sequences and pools the representation of the first token in the sequence. Note that the original BERT model was trained for a masked language model and next-sentence prediction tasks, which includes layers for language model decoding and classification. These layers will not be used for fine-tuning the sentence pair classification.</p>
<p>To help the model distinguish between the two sentences in training, the input is processed in the following way before entering the model:</p>
<pre><code>1 A [CLS] token is inserted at the beginning of the first sentence and a [SEP] token is inserted at the end of each sentence.
2 A sentence embedding indicating Sentence A or Sentence B is added to each token. Sentence embeddings are similar in concept to token embeddings with a vocabulary of 2.
3 A positional embedding is added to each token to indicate its position in the sequence. The concept and implementation of positional embedding are presented in the Transformer paper.
</code></pre>
<h3 id="why-bert">Why BERT?</h3>
<p>1 BERT offers an advantage over models like Word2Vec, because while each word has a fixed representation under Word2Vec regardless of the context within which the word appears, BERT produces word representations that are dynamically informed by the words around them.</p>
<p>2 As opposed to directional models, which read the text input sequentially (left-to-right or right-to-left), the Transformer encoder reads the entire sequence of words at once. Therefore it is considered bidirectional, though it would be more accurate to say that it’s non-directional. This characteristic allows the model to learn the context of a word based on all of its surroundings (left and right of the word).</p>
<h2 id="data">Data</h2>
<p>You can go to <a href="https://www.kaggle.com/crowdflower/twitter-airline-sentiment">here</a> to download the data used for the project.</p>
<p>The <a href="https://www.kaggle.com/bertcarremans/deep-learning-for-sentiment-analysis">deep learning</a> and machine learning to conduct multi-class classification of text can be found <a href="https://www.kaggle.com/tankwin08/how-can-we-predict-the-sentiment-by-tweets/edit">here</a></p>
<h2 id="refereence">Refereence</h2>
<p>1 <a href="http://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/#history">BERT Word Embeddings Tutorial</a></p>
<p>2 <a href="https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270">BERT Explained: State of the art language model for NLP</a></p>

    <p>Project link: <a href="https://github.com/tankwin08/Sentimental-analysis-of-reviewers-feedback-using-BERT-vs-Machien-learning">https://github.com/tankwin08/Sentimental-analysis-of-reviewers-feedback-using-BERT-vs-Machien-learning</a></p>
    <ul class="tags">
    
      <li><a class="tag" href="/tags/nlp">NLP</a></li>
    
      <li><a class="tag" href="/tags/bert">BERT</a></li>
    
      <li><a class="tag" href="/tags/transfer-learning">Transfer learning</a></li>
    
      <li><a class="tag" href="/tags/machine-learning">Machine learning</a></li>
    
      <li><a class="tag" href="/tags/sentiment-analysis">Sentiment analysis</a></li>
    
      <li><a class="tag" href="/tags/tfidf">TFIDF</a></li>
    
</ul>

  </div>
</section>


    <span style="color: #999999; font-size: 60%;">Nifty <a href="https://codepen.io/wbeeftink/pen/dIaDH">tech tag lists</a> from <a class="pen-owner-link" href="https://codepen.io/wbeeftink">Wouter Beeftink</a> </span>
    
  </div>
  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script async src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.0.0/js/bootstrap.bundle.min.js"></script>

  
  <script async src="https://cdnjs.cloudflare.com/ajax/libs/jquery-easing/1.4.1/jquery.easing.min.js"></script>
  
  <script async src="/js/resume.js"></script>
  <script async src="https://www.googletagmanager.com/gtag/js?id="></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', '');
  </script>
  

  
</body>
</html>
