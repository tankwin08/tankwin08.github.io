[{"categories":null,"contents":"Malaria classifiation using CNN and data augumentation Objective: The aim of this project is to distingulish if a person was infeacted with the Malaria from a microscopic image, and provide support for the lab examination results to quickly diagnosis the Malaria parasites.\nMalaria Malaria is caused by Plasmodium parasites. The parasites are spread to people through the bites of infected female Anopheles mosquitoes, called \u0026ldquo;malaria vectors.\u0026rdquo;\nGenerally, the first symptoms of Malaria is fever, headache, and chills, but these symptoms usually appear after 10-15 days after the infective mosquito bite.\nIn addition, the symptoms can be mild or severe and different for children and adult, which make it very difficult to recognize the malaria. For example, children with severe malaria frequently develop one or more of the following symptoms: severe anaemia, respiratory distress in relation to metabolic acidosis, or cerebral malaria. In adults, multi-organ failure is also frequent. In malaria endemic areas, people may develop partial immunity, allowing asymptomatic infections to occur.\nDiagnosis of Malaria can be difficult 1 Where malaria is not endemic any more (such as in the United States), health-care providers may not be familiar with the disease. Clinicians seeing a malaria patient may forget to consider malaria among the potential diagnoses and not order the needed diagnostic tests. Laboratorians may lack experience with malaria and fail to detect parasites when examining blood smears under the microscope.\n2 In some malaria-endemic areas, malaria transmission is so intense that a large proportion of the population is infected but not made ill by the parasites. Such carriers have developed just enough immunity to protect them from malarial illness but not from malarial infection. In that situation, finding malaria parasites in an ill person does not necessarily mean that the illness is caused by the parasites.\nThere are multiple ways to diagnosis the Malaria parasites, such as Clinical diagnosis, Microscopic Diagnosis, Antigen detection, Molecular diagnosis,Serology and Drug resistance tests.\nThus, to develop a automated and robust system to diagnosis the Malaria parasites is emergent and can speed up the traditional way of malaria parasite diagnosis. In this project, we focus on approach named the Microscopic Diagnosis.\nMicroscopic Diagnosis Microscopic Diagnosis is to identify the Malaria parasites by examining under the microscope a drop of the patient’s blood, spread out as a “blood smear” on a microscope slide. Prior to examination, the specimen is stained to give the parasites a distinctive appearance. This technique remains the gold standard for laboratory confirmation of malaria. However, it depends on the quality of the reagents, of the microscope, and on the experience of the laboratorian.\nData The data was obtained from kaggle. Originally, the data was taken from the official NIH Website.\nThere are two folders which contained the infected and uninfected malaria microscopic images.\nMethod summary 1 Download data\r2 Data exploration\r3 Data prepraration\r4 Data split into train, validate and test data sets\r5 Model building and evaluation: original data and data augumentation\r Note: I used my local computer to run this anlysis which took a long time to run, maybe changed to a better computer resources such as Google Colab could save time.\nReferences CDC Microscopic Diagnosis\nWHO Malaria\nMalaria Cell Atlas: mapping a murderous parasite’s life cycle\n","permalink":"/projects/creations/malaria_detection_cnn_healthcare/","tags":["CNN","data augmentation","Malaria","Healthcare"],"title":"Malaria detection using CNN and data augmentation"},{"categories":null,"contents":"Introduction Note: Retraining the BERT model took a long time using a local computer, to run in the Google Colab will be a good choice\nObjectives To predict sentiment (postive, neutral, negeative) of customer feedback using tweet texts of differnt airline companies and compare different models\u0026rsquo;performace on text classification.\nSpecifically, multiple machine learing models such as KNN, Random forest and SVC have been used for conduct classification as a baseline. A new language representation model named BERT (Bidirectional Encoder Representations from Transformers) was also implemented to conduct sentiment analysis.\nBERT Bidirectional Encoder Representations from Transformers (BERT) is a technique for NLP (Natural Language Processing) pre-training developed by Google and published in 2018.\nBERT is a method of pretraining language representations that was used to create models that NLP practicioners can then download and use for free. You can either use these models to extract high quality language features from your text data, or you can fine-tune these models on a specific task (classification, entity recognition, question answering, etc.) with your own data to produce state of the art predictions.\nHow BERT works? BERT was built on the Transformer, an attention mechanism that learns contextual relations between words (or sub-words) in a text. The attention mechanism was used to extratct information of context of a given words and then encode it in a learned vector. Generally, there are two mechanisms - an encoder that reads the text input and a decoder that produces a prediction for the task.\nThe detailed workings of Transformer are described in a paper by Google. The figure below describe the brief steps of BERT during the traning process.\nwhere the model takes a pair of sequences and pools the representation of the first token in the sequence. Note that the original BERT model was trained for a masked language model and next-sentence prediction tasks, which includes layers for language model decoding and classification. These layers will not be used for fine-tuning the sentence pair classification.\nTo help the model distinguish between the two sentences in training, the input is processed in the following way before entering the model:\n1 A [CLS] token is inserted at the beginning of the first sentence and a [SEP] token is inserted at the end of each sentence.\r2 A sentence embedding indicating Sentence A or Sentence B is added to each token. Sentence embeddings are similar in concept to token embeddings with a vocabulary of 2.\r3 A positional embedding is added to each token to indicate its position in the sequence. The concept and implementation of positional embedding are presented in the Transformer paper.\r Why BERT? 1 BERT offers an advantage over models like Word2Vec, because while each word has a fixed representation under Word2Vec regardless of the context within which the word appears, BERT produces word representations that are dynamically informed by the words around them.\n2 As opposed to directional models, which read the text input sequentially (left-to-right or right-to-left), the Transformer encoder reads the entire sequence of words at once. Therefore it is considered bidirectional, though it would be more accurate to say that it’s non-directional. This characteristic allows the model to learn the context of a word based on all of its surroundings (left and right of the word).\nData You can go to here to download the data used for the project.\nThe deep learning and machine learning to conduct multi-class classification of text can be found here\nRefereence 1 BERT Word Embeddings Tutorial\n2 BERT Explained: State of the art language model for NLP\n","permalink":"/projects/creations/sentimental-analysis-of-reviewers-feedback-using-bert/","tags":["NLP","BERT","Transfer learning","Machine learning","Sentiment analysis","TFIDF"],"title":"Sentimental analysis of reviewers' feedback using BERT vs. Machien learning"},{"categories":null,"contents":"Introduction Goal To examplify the feature selection strategy in PySpark and furhter enhanc the pyspark pipeline\u0026rsquo;s performance on fraud detection.\nIn this project, I will continue to work on the data from the project fradu_detection_ML_PySpark. The data exploration will be same, and the feature selction will use input perturbation strtegry instead of PCA as I did in the previous project.\nWhy feature selection? 1. Curse of dimensionality — Overfitting\nThe common theme of the problems is that when the dimensionality increases, the volume of the space increases so fast that the available data become sparse. This sparsity is problematic for any method that requires statistical significance.\nIn order to obtain a statistically sound and reliable result, the amount of data needed to support the result often grows exponentially with the dimensionality.\nAlso, organizing and searching data often relies on detecting areas where objects form groups with similar properties; in high dimensional data, however, all objects appear to be sparse and dissimilar in many ways, which prevents common data organization strategies from being efficient.\n2. Occam’s Razor\nWe want our models to be simple and explainable. We lose explainability when we have a lot of features.\n3 Noise in the data\nIn real applications, the data are not perfect and always noisy inherently.\nCommonly used methods for feature selection In summary, there are several commonly used methods to conduct feature selction in data preprocessing.\n1 Correlation or chi-square\nChose the top-n high correlated variables or high chi-squre variables with respective to target variables. The intuition is that if a feature is independent to the target, it will not be useful or uninformative for target classification or regression.\n2 Stepwise method\nThis is a wrapper based method. The goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.\n3 Lasso - Penalized likelihood\nLASSO models have been used extensively in high-dimensional model selection problems, that is when the number of IVs 𝑘 by far exceeds the sample size 𝑛.\nRegression coefficients estimated by the LASSO are biased by intention, but can have smaller mean squared error (MSE) than conventional estimates. It prefer to have fewer variales with huge contribution to the target.\nBecause of the bias, their interpretation in explanatory or descriptive models is difficult, and confidence intervals based on resampling procedures such as the percentile bootstrap do not reach their claimed nominal level. Another problem with LASSO estimation is its dependence on the scale of the covariates\n4 PCA\nPCA is a commonly used as dimension reduction technique by projecting each data point onto only the first few principal components to obtain lower-dimensional data while preserving as much of the data\u0026rsquo;s variation as possible.\nThe advantages of PCA:\n Removes Correlated Features Improves Algorithm Performance Reduces Overfitting Improves Visualization  The things we need to consider before using PCA:\n Independent variables become less interpretable: Data standardization is must before PCA: pca is affected by scale Information loss  5 Input Perturbation\nThis algorithm was introduced by Breiman in his seminal paper on random forests. Although he presented this algorithm in conjunction with random forests, it is model-independent and appropriate for any supervised learning model.\nThis algorithm, known as the input perturbation algorithm, works by evaluating a trained model’s accuracy with each of the inputs individually shuffled from a data set. Shuffling an input causes it to become useless—effectively removing it from the model. More important inputs will produce a less accurate score when they are removed by shuffling them. This process makes sense, because important features will contribute to the accuracy of the model.\n","permalink":"/projects/creations/enhanced_pyspark_ml_fraud_detection_feature_selection/","tags":["pipeline","ML","Hyperparameter tunning","PySpark","Fraud detection","Feature importance"],"title":"Enhanced fraud detection using ML and PySpark framework with feature selection"},{"categories":null,"contents":"Introduction The image on the right was obtained from here.\nThere is a lack of public available datasets on financial services and specially in the emerging mobile money transactions domain. Financial datasets are important to many researchers and in particular to us performing research in the domain of fraud detection. Part of the problem is the intrinsically private nature of financial transactions, that leads to no publicly available datasets.\nA synthetic dataset were generated using the simulator called PaySim. PaySim uses aggregated data from the private dataset to generate a synthetic dataset that resembles the normal operation of transactions and injects malicious behaviour to later evaluate the performance of fraud detection methods.\nData cleaning and understanding The data is big and we did a comprehensive data exploration on the data to find the pattern existed in the data. For example, to investigate which types of transactions are more likely to the fraud.\nIn addition, the data is always noisy inherently and how to identify them is also crucial for building a generalized model.\nIn this project, the feature enginering have been used for outlier detection.\nHyperparameter tunning in PySpark The hyperparamters are critical to build a robust and high performing models. Generally, this step is one of trickiest part of ML pipeline. The goal of hyperparameter tuning is to optimize the model and achieve a better model performance. The most common approaches including grid search and random search to do hyperparameter tuning in PySpark is the same as we did in a typical-classical machine learning pipeline (scikit learn).\nTo avoid the overfitting issue, different data split strategies and cross validation were also used. The users can chose which way they prefer to conduct a similar study or explore both and to see which way can give a better solutions.\nReferences PySpark ML Tuning: model selection and hyperparameter tuning.\n","permalink":"/projects/creations/pyspark_ml_fraud_detection/","tags":["Big data","Machine learning","Hyperparameter tunning","PySpark","Fraud detection","Imblanced data"],"title":"Fraud detection using ML and PySpark framework"},{"categories":null,"contents":"Introduction Goal To examplify the uses of ensemble models in PySpark as the ensemble models in previous project using sklearn and keras and predict if the client will subscribe (yes/no) a term deposit (variable y) using market campaign data.\nEnsemble models Ensemble modeling is a process where multiple diverse models are created to predict an outcome, either by using many different modeling algorithms or using different training data sets. The ensemble model then aggregates the prediction of each base model and results in once final prediction for the unseen data. The motivation for using ensemble models is to reduce the generalization error of the prediction. As long as the base models are diverse and independent, the prediction error of the model decreases when the ensemble approach is used.\nThe approach seeks the wisdom of crowds in making a prediction. Even though the ensemble model has multiple base models within the model, it acts and performs as a single model.\nA signle model generally suffers from high bias or high variance due to data quality, train and test data drift, distribution of hypothesis\u0026hellip;\nThe goal of the modelling to find a method wit low bias and low variance. Thus, it is common to aggregate several base models to provide solutions.\nAggregate strategies There are multiple ways to conduct aggregation and improve the model performance either from accuracy or robustness.\n1 Baggging\nThe bagging strategy is built on the bootstrap sampling. In short, it built multiple classifer independently and in parallel using data derived from resampling from the training set. Then aggregate these classifiers using average processing or major vote to redce the variability of prediction. However the accuracy/point estimate is not improved.\n2 Boosting\nThe boosting is similar to bagging strategy to some extent in terms of resampling methods. But it differs in two major ways:\n1 how trees are built: The Bagging method builds each tree independently while Boosting method builds one tree at a time. This additive model (ensemble) works in a forward stage-wise manner, introducing a weak learner to improve the shortcomings of existing weak learners. 2 Results combination: The Bagging method combine results at the end of the process (by averaging or \u0026quot;majority rules\u0026quot;) while the boosting combines results along the way.\r If you carefully tune parameters, boosting can result in better performance than bagging. However, boosting may not be a good choice if you have a lot of noise, as it can result in overfitting. They also tend to be harder to tune than bagging method.\n3 Stacking\nStacking provide a whole new different way to combine classifers. There are two major differences:\n1 stacking often considers heterogeneous weak learners (different learning algorithms are combined) whereas bagging and boosting consider mainly homogeneous weak learners.\r2 The stacking uses a second layer model which uses the predictions of weak classifiers such as bagging and bostting results as input.\r In this project, the stacking strategy was used to predict if the client will subscribe (yes/no) a term deposit (variable y) using market campaign data.\nData The data used for this project can be downloaded from here.\nThe explination of the data can be found here.\nThe data is related with direct marketing campaigns of a Portuguese banking institution. The marketing campaigns were based on phone calls. Often, more than one contact to the same client was required, in order to access if the product (bank term deposit) would be (\u0026lsquo;yes\u0026rsquo;) or not (\u0026lsquo;no\u0026rsquo;) subscribed.\nInput variables: bank client data:\n1 - age (numeric)\r2 - job : type of job (categorical: 'admin.','blue-collar','entrepreneur','housemaid','management','retired','self-employed','services','student','technician','unemployed','unknown')\r3 - marital : marital status (categorical: 'divorced','married','single','unknown'; note: 'divorced' means divorced or widowed)\r4 - education (categorical: 'basic.4y','basic.6y','basic.9y','high.school','illiterate','professional.course','university.degree','unknown')\r5 - default: has credit in default? (categorical: 'no','yes','unknown')\r6 - housing: has housing loan? (categorical: 'no','yes','unknown')\r7 - loan: has personal loan? (categorical: 'no','yes','unknown')\r related with the last contact of the current campaign:\n8 - contact: contact communication type (categorical: 'cellular','telephone') 9 - month: last contact month of year (categorical: 'jan', 'feb', 'mar', ..., 'nov', 'dec')\r10 - day_of_week: last contact day of the week (categorical: 'mon','tue','wed','thu','fri')\r11 - duration: last contact duration, in seconds (numeric).  Important note: Attribute 11 highly affects the output target (e.g., if duration=0 then y='no\u0026rsquo;). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. Thus, this input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model.\nother attributes:\n12 - campaign: number of contacts performed during this campaign and for this client (numeric, includes last contact)\r13 - pdays: number of days that passed by after the client was last contacted from a previous campaign (numeric; 999 means client was not previously contacted)\r14 - previous: number of contacts performed before this campaign and for this client (numeric)\r15 - poutcome: outcome of the previous marketing campaign (categorical: 'failure','nonexistent','success')\r social and economic context attributes\n16 - emp.var.rate: employment variation rate - quarterly indicator (numeric)\r17 - cons.price.idx: consumer price index - monthly indicator (numeric) 18 - cons.conf.idx: consumer confidence index - monthly indicator (numeric) 19 - euribor3m: euribor 3 month rate - daily indicator (numeric)\r20 - nr.employed: number of employees - quarterly indicator (numeric)\r Output variable (desired target):\n21 - y - has the client subscribed a term deposit? (binary: 'yes','no')\r References Machine Learning Case Study with Spark: Make it better.\n","permalink":"/projects/creations/ensemble_model_pyspark/","tags":["pipeline","ML","Ensemble model","PySpark","Market campaign","Feature importance"],"title":"Ensemble models in PySpark"},{"categories":null,"contents":"Introduction Bayesian NN How much confidence do you know about your model results or a paritcular prediction?\nThis is a critical important question for many business. With the advent of deep learning, many forecasting problems for business have been solved in innovative ways. For example, Uber researchers has provided a fascianting paper on time series prediction.\nStandard deep learning method such as LSTM do not capture model uncertianty. However, the uncertianty estimation is indispensable for deep learning models.\nBayesian probability theory offers us mathematically grounded tools to reason about model uncertainty, but these usually come with a prohibitive computational cost [2].\nIn deep learning, there are two kinds of strategries to quantify the uncertianty: (1) MC dropout and (2) variational inference.\n(1) Regarding MC dropout, Gal developed a framework casting dropout training in deep neural networks (NNs) as approximate Bayesian inference in deep Gaussian processes. This method can mitigates the problem of representing model uncertainty in deep learning without sacrificing either computational complexity or test accuracy.\n(2)Variational inference such as sampling-based variational inference and stochastic variational inference has been applied to deep learning models, which have performed as well as dropout. However, this approach comes with a prohibitive computational cost. To represent uncertainty, the number of parameters in these appraoches is doubled for the same network size. Further, they require more time to converge and do not improve on existing techniques. Given that good uncertainty estimates can be cheaply obtained from common dropout models, this might result in unnecessary additional computation.\nwhat is variational inference? In short, variational inference is an approach of approximating model posterior which would otherwise be difficult to work with directly. Intuitively, this is a measure of similarity between the two distributions although it is not symmetric. So minimising this objective fits our approximating distribution to the distribution we care about.\nThis is standard in variational inference where we fit distributions rather than parameters, resulting in our robustness to over-fitting.\nHow dropout to represent Bayesian approximation Compared to standard NN, the BNN added a binary vector in each layer. We sample new realisations for the binary vectors bi for every input point and every forward pass thorough the model (evaluating the model\u0026rsquo;s output), and use the same values in the backward pass (propagating the derivatives to the parameters to be optimised W1,W2,b). The elements of vector bi take value 1 with probability 0≤pi≤1 for i=1,2\u0026hellip;l. i is the ith layer.\nThe dropped weights b1W1 and b2W2 are often scaled by 1/pi to maintain constant output magnitude. At test time we do not sample any variables and simply use the full weights matrices W1,W2,b.\nActually, the dropout network is similar to a Gaussian process approximation. Different network structures and different non-linearities would correspond to different prior beliefs as to what we expect our uncertainty to look like. This property is shared with the Gaussian process as well. Different Gaussian process covariance functions would result in different uncertainty estimates. If you are interested in more details on the BNN. Please refer to [here]（http://mlg.eng.cam.ac.uk/yarin/blog_3d801aa532c1ce.html） - Why Does It Even Make Sense?\nCalculating prediction uncertainty with BNNs developed by Uber The variance quantifies the prediction uncertainty, which can be broken down using the law of total variance. An underlying assumption for the model uncertainty equation is that yhat is generated by the same procedure, but this is not always the case. In anomaly detection, for instance, it is expected that certain time series will have patterns that differ greatly from the trained model. Therefore, reseachers from Uber propose that a complete measurement of prediction uncertainty should be composed of three parts:\n1 model uncertainty\n2 model misspecification\n3 inherent noise level.\n1 Model Uncertainty Model uncertainty, also referred to as epistemic uncertainty, captures our ignorance of the model parameters and can be reduced as more samples are collected. The key to estimating model uncertainty is the posterior distribution , also referred to as Bayesian inference. This is particularly challenging in neural networks because of the non-conjugacy often caused by nonlinearities.\nHere we used Monte Carlo dropout to approximate model uncertainty.\n2 Model misspecification Model misspecification captures the scenario where testing samples come from a different population than the training set, which is often the case in time series anomaly detection. Similar concepts have gained attention in deep learning under the concept of adversarial examples in computer vision, but its implication in prediction uncertainty remains relatively unexplored\nHere we first fit a latent embedding space for all training time series using an encoder-decoder framework. From there, we are able to measure the distance between test cases and training samples in the embedded space.\nAfter estimating uncertaitny from model misspecification, we combined model uncertianty with model misspecification uncertianty by connecting the encoder-decoder network with a prediction network, and treat them as one large network during inference.\n3 Inherent noise Inherent noise is mainly to capture the uncertainty in the data generation process and which is irreducible. Uber reseachers propose a simple but adaptive approach by estimating the noise level via the residual sum of squares, evaluated on an independent held-out validation set.\nIf you are interested in technical parts on these three sections, you can go to here for more details.\n","permalink":"/projects/creations/bayesian-uncertainty-of-neutral-networks-lstm-for-time-series-analysis/","tags":["Time series","LSTM","Bayesian Uncertainty","Autoencoder","Dropout","Model misspecification","Inherent noise"],"title":"Bayesian Uncertainty for time series data (EVI) prediction using LSTM and autoencoder"},{"categories":null,"contents":"Introduction ARIMA (Autoregressive integrated moving average) ARIMA can explain the time series pattern for given frequency or lag (hour, day and week \u0026hellip;) and also predict furhter values.\nARIMA analysis will focus on the logic of model and how to select the three important terms of the model: p is the order of the AR term,\nq is the order of the MA term, d is the number of differencing required to make the time series stationary. You can go to here for detailed explaination of ARIMA.\nThe first step of the model is to make the time series data stationary.\nStationarity\nwhat is stationary?\n1 The mean of the series should not be a function of time.\n2 The variance of the series should not be a function of time. This property is known as homoscedasticity.\n3 the covariance of the i th term and the (i + m) th term should not be a function of time\nwhy the data need to be stationary?\nBecause, term ‘Auto Regressive’ in ARIMA means it is a linear regression model that uses its own lags as predictors.\nWhen running a linear regression the assumption is that all of the observations are all independent of each other. In a time series, however, we know that observations are time dependent. It turns out that a lot of nice results that hold for independent random variables (law of large numbers and central limit theorem to name a couple) hold for stationary random variables. So by making the data stationary, we can actually apply regression techniques to this time dependent variable.\nHow to check Stationarity?\nAn augmented Dickey–Fuller test (ADF) tests the null hypothesis that a unit root is present in a time series sample. The alternative hypothesis is different depending on which version of the test is used, but is usually stationarity or trend-stationarity.\nBasically, we are trying to whether to accept the Null Hypothesis H0 (that the time series has a unit root, indicating it is non-stationary) or reject H0 and go with the Alternative Hypothesis (that the time series has no unit root and is stationary).\nWe end up deciding this based on the p-value return.\n •A small p-value (typically ≤ 0.05) indicates strong evidence against the null hypothesis, so you reject the null hypothesis.\r•A large p-value (\u0026gt; 0.05) indicates weak evidence against the null hypothesis, so you fail to reject the null hypothesis.\r Parameters of ARIMA\nHow to make data stationary and determine d?\nThe most common way is to difference it by substracting the previous values from the current values. This is also how to determine the d.\nIf d = 0, the input data of the model will be original data. If d = 1, it means we need to use the first difference of the time seriese.\nNext, what are the ‘p’ and ‘q’ terms?\n‘p’ is the order of the ‘Auto Regressive’ (AR) term. It refers to the number of lags of Y to be used as predictors. And ‘q’ is the order of the ‘Moving Average’ (MA) term. It refers to the number of lagged forecast errors that should go into the ARIMA Model.\nHow to determine p and q?\n  Identification of an AR model is often best done with inspecting the Partial Autocorrelation (PACF) plot.\n For an AR model, the theoretical PACF “shuts off” past the order of the model. The phrase “shuts off” means that in theory the partial autocorrelations are equal to 0 beyond that point. Put another way, the number of non-zero partial autocorrelations gives the order of the AR model. By the “order of the model” we mean the most extreme lag of x that is used as a predictor.  how to determine p: In particular, the partial autocorrelation at lag k is equal to the estimated AR(k) coefficient in an autoregressive model with k terms\u0026ndash;i.e., a multiple regression model in which Y is regressed on LAG(Y,1), LAG(Y,2), etc., up to LAG(Y,k). Thus, by mere inspection of the PACF you can determine how many AR terms you need to use to explain the autocorrelation pattern in a time series: if the partial autocorrelation is significant at lag k and not significant at any higher order lags\u0026ndash;i.e., if the PACF \u0026ldquo;cuts off\u0026rdquo; at lag k\u0026ndash;then this suggests that you should try fitting an autoregressive model of order k\n  Identification of an MA model is often best done with the ACF rather than the PACF.\n For an MA model, the theoretical PACF does not shut off, but instead tapers toward 0 in some manner. A clearer pattern for an MA model is in the ACF. The ACF will have non-zero autocorrelations only at lags involved in the model.  how to determine q: to see first negative values of ACF plot\u0026rsquo;s corresponding x-axis (lag) value\n  Note: The sole reliance on the ACF and PACF to determine the p and q is NOT always correct. These plots just can help you understnad the ARIMA model and justify whether you can get a better model or worse one. Just try the simple model first and find a better model using AIC through multiple time exploration.\n","permalink":"/projects/creations/arima_vs._lstm_time_series_data/","tags":["Time series","ARIMA","Seasonal ARIMA","Stationary","LSTM","RISK prediction"],"title":"Time series analysis using ARIMA \u0026 LSTM - MODIS"},{"categories":null,"contents":"Introduction:\nGenealy, it require an amount of data to train and a long time to train a NLP model for a specific dataset. Transfer learning is commonly used in this case to conduct the sentiment analsis for Natural Language Processing (NLP) problem.\nSwivel performs approximate factorization of the point-wise mutual information matrix via stochastic gradient descent. It uses a piecewise loss with special handling for unobserved co-occurrences, and thus makes use of all the information in the matrix.\nPretrained Word Embeddings Pretrained Word Embeddings are the embeddings learned in one task that are used for solving another similar task.\nThese embeddings are trained on large datasets, saved, and then used for solving other tasks. That’s why pretrained word embeddings are a form of Transfer Learning.\nCBOW (Continuous Bag Of Words) and Skip-Gram are two most popular frames for word embedding. In CBOW the words occurring in context (surrounding words) of a selected word are used as inputs and middle or selected word as the target. Its the other way round in Skip-Gram, here the middle word tries to predict the words coming before and after it.\nWhy do we need Pretrained Word Embeddings?\nPretrained word embeddings capture the semantic and syntactic meaning of a word as they are trained on large datasets. They are capable of boosting the performance of a Natural Language Processing (NLP) model. These word embeddings come in handy during hackathons and of course, in real-world problems as well.\nBut why should we not learn our own embeddings? Well, learning word embeddings from scratch is a challenging problem due to two primary reasons:\n  1 Sparsity of training data\n  2 Large number of trainable parameters\n  ","permalink":"/projects/creations/nlp_swivel/","tags":["NLP","SWIVEL","Transfer learning","Embedding layer","Sentiment analysis"],"title":"Sentiment analysis for review classification using SWIVEL and a small datasets"},{"categories":null,"contents":"Introduction:\nEnsemble modeling is a process where multiple diverse models are created to predict an outcome, either by using many different modeling algorithms or using different training data sets. The ensemble model then aggregates the prediction of each base model and results in once final prediction for the unseen data. The motivation for using ensemble models is to reduce the generalization error of the prediction. As long as the base models are diverse and independent, the prediction error of the model decreases when the ensemble approach is used.\nThe approach seeks the wisdom of crowds in making a prediction. Even though the ensemble model has multiple base models within the model, it acts and performs as a single model.\nWhy Ensemble modeling?\nThere are two major benefits of Ensemble models:\n1 Better prediction\r2 More stable model\r The aggregate opinion of a multiple models is less noisy than other models. In finance, it was called “Diversification”, a mixed portfolio of many stocks will be much less variable than just one of the stocks alone.\nThis is also why your models will be better with ensemble of models rather than individual. One of the caution with ensemble models are over fitting although bagging takes care of it largely.\n","permalink":"/projects/creations/ensemble_models_classification/","tags":["Ensemble model","Optimized NN","Machine learning","Robust to noise","Deep learning"],"title":"Ensemble models for classification (combine deep learning with machine learning)"},{"categories":null,"contents":"Make waveform lidar processing become easier and enable users can use exisitng Discrete-return lidar data processing tools.\nIntroduction:\nA wealth of Full Waveform (FW) LiDAR data are available to the public from different sources, which is poised to boost the extensive application of FW LiDAR data. However, we lack a handy and open source tool that can be used by potential users for processing and analyzing FW LiDAR data. To this end, we introduce waveformlidar, an R package dedicated to FW LiDAR processing, analysis and visualization as a solution to the constraint. Specifically, this package provides several commonly used waveform processing methods such as Gaussian, adaptive Gaussian and Weibull decompositions, and deconvolution approaches (Gold and Richard-Lucy (RL)) with users customized settings. In addition, we also develop some functions to derive commonly used waveform metrics for characterizing vegetation structure. Moreover, a new way to directly visualize FW LiDAR data is developed through converting waveforms into points to form the Hyper Point cloud (HPC), which can be easily adopted and subsequently analyzed with existing discrete-return LiDAR processing tools such as LAStools and FUSION. Basic explorations of the HPC such as 3D voxelization of the HPC and conversion from original waveforms to composite waveforms are also available in this package. All of these functions are developed based on small-footprint FW LiDAR data, but they can be easily transplanted to the large footprint FW LiDAR data such as Geoscience Laser Altimeter System (GLAS) and Global Ecosystem Dynamics Investigation (GEDI) data analysis.\n","permalink":"/projects/contributions/waveformlidar_r_package/","tags":["R","Waveformlidar","NEON","Decompostion and deconvolution","hyper point cloud","Gaussian","Weibull","Adaptive Gaussian","percentile height/intensity"],"title":"R package - waveformlidar data processing and analysis"},{"categories":null,"contents":"Introduction:\nThe picture of Bayesian optimization is obtianed from here\nBayesian optimization\nThere are a lot of hyperparameters for machine learning models such as NN. Typically, random or grid search are effecient ways to conduct the optimization of models. They can be very time-consuming in some cases which waste time on unpromising areas of search space. Bayesian optimization can overcome this problem by adopting an informed seach method in the space to find the optmized parameters.\nBayesian optimization works by constructing a posterior distribution of functions (gaussian process) that best describes the function you want to optimize. As the number of observations grows, the posterior distribution improves, and the algorithm becomes more certain of which regions in parameter space are worth exploring and which are not. You can find more information and explination here\nIt\u0026rsquo;s worthy to note that the Bayesian optimization is to find the maximum of a function. Thus, when we formulate a function and evaulation metrics, we should take this part into consideration. For example, when we used log loss to evaluate our model performance, the smaller values will be better. We should return a negative logloss to make it suitable for maximum of the defined function.\nNote: It took a long time to run if you have a big dataset and wide boundary. You can refer to Colab for running the code.\n","permalink":"/projects/creations/bayesian_optimziation_nn/","tags":["Neutral Networks","Bayesian optimization","Classification","Architecture Optimization"],"title":"Bayesian optimization deep learning"},{"categories":null,"contents":"Make waveform lidar processing become easier and enable users can use exisitng Discrete-return lidar data processing tools.\nIntroduction:\nA wealth of Full Waveform (FW) LiDAR data are available to the public from different sources, which is poised to boost the extensive application of FW LiDAR data. However, we lack a handy and open source tool that can be used by potential users for processing and analyzing FW LiDAR data. To this end, we introduce waveformlidar, an R package dedicated to FW LiDAR processing, analysis and visualization as a solution to the constraint. Specifically, this package provides several commonly used waveform processing methods such as Gaussian, adaptive Gaussian and Weibull decompositions, and deconvolution approaches (Gold and Richard-Lucy (RL)) with users customized settings. In addition, we also develop some functions to derive commonly used waveform metrics for characterizing vegetation structure. Moreover, a new way to directly visualize FW LiDAR data is developed through converting waveforms into points to form the Hyper Point cloud (HPC), which can be easily adopted and subsequently analyzed with existing discrete-return LiDAR processing tools such as LAStools and FUSION. Basic explorations of the HPC such as 3D voxelization of the HPC and conversion from original waveforms to composite waveforms are also available in this package. All of these functions are developed based on small-footprint FW LiDAR data, but they can be easily transplanted to the large footprint FW LiDAR data such as Geoscience Laser Altimeter System (GLAS) and Global Ecosystem Dynamics Investigation (GEDI) data analysis.\n","permalink":"/projects/creations/waveformlidar_r_package/","tags":["R","Waveformlidar","NEON","Decompostion and deconvolution","hyper point cloud","Gaussian","Weibull","Adaptive Gaussian","percentile height/intensity"],"title":"R package - waveformlidar data processing and analysis"},{"categories":null,"contents":"Goal:\nMake waveform lidar processing become easier and enable users can use exisitng Discrete-return lidar data processing tools.\nIntroduction:\nWaveform Light Detection and Ranging (LiDAR) data have advantages over discrete-return LiDAR data in accurately characterizing vegetation structure. However, we lack a comprehensive understanding of waveform data processing approaches under different topography and vegetation conditions. The objective of this paper is to highlight a novel deconvolution algorithm, the Gold algorithm, for processing waveform LiDAR data with optimal deconvolution parameters. Further, we present a comparative study of waveform processing methods to provide insight into selecting an approach for a given combination of vegetation and terrain characteristics. We employed two waveform processing methods: (1) direct decomposition, (2) deconvolution and decomposition. In method two, we utilized two deconvolution algorithms – the Richardson-Lucy (RL) algorithm and the Gold algorithm. The comprehensive and quantitative comparisons were conducted in terms of the number of detected echoes, position accuracy, the bias of the end products (such as digital terrain model (DTM) and canopy height model (CHM)) from the corresponding reference data, along with parameter uncertainty for these end products obtained from different methods. This study was conducted at three study sites that include diverse ecological regions, vegetation and elevation gradients. Results demonstrate that two deconvolution algorithms are sensitive to the pre-processing steps of input data. The deconvolution and decomposition method is more capable of detecting hidden echoes with a lower false echo detection rate, especially for the Gold algorithm. Compared to the reference data, all approaches generate satisfactory accuracy assessment results with small mean spatial difference (\u0026lt;1.22 m for DTMs, \u0026lt;0.77 m for CHMs) and root mean square error (RMSE) (\u0026lt;1.26 m for DTMs, \u0026lt;1.93 m for CHMs). More specifically, the Gold algorithm is superior to others with smaller root mean square error (RMSE) (\u0026lt;1.01 m), while the direct decomposition approach works better in terms of the percentage of spatial difference within 0.5 and 1 m. The parameter uncertainty analysis demonstrates that the Gold algorithm outperforms other approaches in dense vegetation areas, with the smallest RMSE, and the RL algorithm performs better in sparse vegetation areas in terms of RMSE. Additionally, the high level of uncertainty occurs more on areas with high slope and high vegetation. This study provides an alternative and innovative approach for waveform processing that will benefit high fidelity processing of waveform LiDAR data to characterize vegetation structures.\n","permalink":"/projects/creations/waveformlidar_processing/","tags":["Waveformlidar","Decompostion and deconvolution","Gaussian","Weibull","Adaptive Gaussian","Gold/RL"],"title":"waveform decomposition vs. deconvolution"},{"categories":null,"contents":"Introduction:\nSmall unmanned aerial systems (UAS) have emerged as high-throughput platforms for the collection of high-resolution image data over large crop fields to support precision agriculture and plant breeding research. At the same time, the improved efficiency in image capture is leading to massive datasets, which pose analysis challenges in providing needed phenotypic data.\nTo complement these high-throughput platforms, there is an increasing need in crop improvement to develop robust image analysis methods to analyze large amount of image data. Analysis approaches based on deep learning models are currently the most promising and show unparalleled performance in analyzing large image datasets.\nGoal:\nThis study developed and applied an image analysis approach based on a SegNet deep learning semantic segmentation model to estimate sorghum panicles counts, which are critical phenotypic data in sorghum crop improvement, from UAS images over selected sorghum experimental plots.\nMethods:\nThe SegNet model was trained to semantically segment UAS images into sorghum panicles, foliage and the exposed ground using 462, 250 * 250 labeled images, which was then applied to field orthomosaic to generate a field-level semantic segmentation.\nHighlights:\n(1) post processing\nIndividual panicle locations were obtained after post-processing the segmentation output to remove small objects and split merged panicles. A comparison between model panicle count estimates and manually digitized panicle locations in 60 randomly selected plots showed an overall detection accuracy of 94%.\n(2) plot level\nA per-plot panicle count comparison also showed high agreement between estimated and reference panicle counts (Spearman correlation r = 0.88, mean bias = 0.65).\n(3) where is the error coming from?\nMisclassifications of panicles during the semantic segmentation step and mosaicking errors in the field orthomosaic contributed mainly to panicle detection errors.\nOverall, the approach based on deep learning semantic segmentation showed good promise and with a larger labeled dataset and extensive hyper-parameter tuning, should provide even more robust and effective characterization of sorghum panicle counts.\n","permalink":"/publications/deep_learning_count_panicles/","tags":["deep learning","semantic segmentation","sorghum panicle","plant phenotyping","unmanned aerial systems (UAS)","plant breeding","automation","counting"],"title":"A Deep Learning Semantic Segmentation-Based Approach for Field-Level Sorghum Panicle Counting"},{"categories":null,"contents":"Introduction:\nA plethora of information contained in full-waveform (FW) Light Detection and Ranging (LiDAR) data offers prospects for characterizing vegetation structures.\nGoal:\nThis study aims to investigate the capacity of FW LiDAR data alone for tree species identification through the integration of waveform metrics with machine learning methods and Bayesian inference.\nMethods:\nSpecifically, we first conducted automatic tree segmentation based on the waveform-based canopy height model (CHM) using three approaches including TreeVaW, watershed algorithms and the combination of TreeVaW and watershed (TW) algorithms.\nSubsequently, the Random forests (RF) and Conditional inference forests (CF) models were employed to identify important tree-level waveform metrics derived from three distinct sources, such as raw waveforms, composite waveforms, the waveform-based point cloud and the combined variables from these three sources.\nFurther, we discriminated tree (gray pine, blue oak, interior live oak) and shrub species through the RF, CF and Bayesian multinomial logistic regression (BMLR) using important waveform metrics identified in this study.\nHighlights:\n(1) tree segmentation:\nResults of the tree segmentation demonstrated that the TW algorithms outperformed other algorithms for delineating individual tree crowns.\n(2) RF vs. CF\nThe CF model overcomes waveform metrics selection bias caused by the RF model which favors correlated metrics and enhances the accuracy of subsequent classification.\n(3) Composite waveform vs. raw waveform\nWe also found that composite waveforms are more informative than raw waveforms and waveform-based point cloud for characterizing tree species in our study area.\n(4) RF vs. Bayesian\nBoth classicalmachine learning methods (the RF and CF) and the BMLR generated satisfactory average overall accuracy (74% for the RF, 77% for the CF and 81% for the BMLR) and the BMLR slightly outperformed the other two methods.\nHowever, these three methods suffered from low individual classification accuracy for the blue oak which is prone to being misclassified as the interior live oak due to the similar characteristics of blue oak and interior live oak.\nUncertainty estimates from the BMLR method compensate for this downside by providing classification results in a probabilistic sense and rendering users with more confidence in interpreting and applying classification results to real-world tasks such as forest inventory.\n(5) feature selection:\nOverall, this study recommends the CF method for feature selection and suggests that BMLR could be a superior alternative to classical machining learning methods.\n","permalink":"/publications/bayesian_machine_learning_new/","tags":["Bayesian multinomial logistic regression","Conditional inference forests","Machine learning, Random forests","Waveform signatures","Tree segmentation","Watershed","composite waveform"],"title":"Bayesian and Classical Machine Learning Methods: A Comparison for Tree Species Classification with LiDAR Waveform Signatures"},{"categories":null,"contents":"Introduction:\nA wealth of FullWaveform (FW) LiDAR (Light Detection and Ranging) data are available to the public from different sources, which is poised to boost extensive applications of FW LiDAR data. However, we lack a handy and open source tool that can be used by potential users for processing and analyzing FW LiDAR data.\nGoal: To this end, we introduce waveformlidar, an R package dedicated to FW LiDAR processing, analysis and visualization as a solution to the constraint.\nWhat this package provide:\n(1) This package provides several commonly used waveform processing methods such as Gaussian, Adaptive Gaussian and Weibull decompositions and deconvolution approaches (Gold and Richard-Lucy (RL)) with users\u0026rsquo; customized settings.\n(2) we also developed functions to derive commonly used waveform metrics for characterizing vegetation structure.\n(3) Moreover, a new way to directly visualize FW LiDAR data is developed by converting waveforms into points to form the Hyper Point Cloud (HPC), which can be easily adopted and subsequently analyzed with existing discrete-return LiDAR processing tools such as LAStools and FUSION.\n(4) Basic explorations of the HPC such as 3D voxelization of the HPC and conversion from original waveforms to composite waveforms are also available in this package.\nAll of these functions are developed based on small-footprint FW LiDAR data but they can be easily transplanted to the large footprint FW LiDAR data such as Geoscience Laser Altimeter System (GLAS) and Global Ecosystem Dynamics Investigation (GEDI) data analysis.\nIt is anticipated that these functions will facilitate the widespread use of FW LiDAR and be beneficial for better estimating biomass and characterizing vegetation structure at various scales.\n","permalink":"/publications/waveformlidar_r_package/","tags":["waveform decomposition","hyper point cloud","deconvolution","Waveform signatures","waveform gridding \u0026 voxel","composite waveform"],"title":"waveformlidar: An R Package for Waveform LiDAR Processing and Analysis"},{"categories":null,"contents":"Introduction:\nThe Ice, Cloud and Land Elevation Satellite-2 (ICESat-2) launched on September 15th, 2018 and this mission offers an extraordinary opportunity to contribute to an assessment of forest resources at multiple spatial scales. This study served to develop a methodology for utilizing ICESat-2 data over vegetated areas.\nGoal: The specific objectives were to: (1) derive a simulated ICESat-2 photon-counting lidar (PCL) vegetation product using airborne lidar data,\n(2) examine the use of simulated PCL metrics for modelling aboveground biomass (AGB) along ICESat-2 profiles using a simulated ICESat-2 PCL vegetation product and reference AGB estimated from airborne lidar data, and\n(3) estimate forest canopy cover using simulated PCL canopy product data and airborne lidarderived canopy cover.\nWhere and scenarios: Using existing airborne lidar data for Sam Houston National Forest (SHNF) in Texas and known ICESat-2 track locations, PCL simulations were carried out. Three scenarios were analyzed in this study;\n  simulated data without the addition of noise,\n  processed simulated data for daytime, and\n  nighttime scenarios.\n  Highlights:\nproduct Segments measuring 100m along the proposed ICESat-2 tracks were used to extract simulated PCL metrics from each of the three data scenarios and spatially coincident, reference airborne lidar-estimated AGB and airborne lidar canopy cover estimates.\nLinear regression models were then developed with a subset of the simulated PCL segments to estimate AGB and canopy cover and their performance assessed using separate testing sets.\nThree scenarios: AGB model testing with the simulated dataset without noise, nighttime and daytime scenarios resulted in R2 values of 0.79, 0.79 and 0.63 respectively, with root mean square error (RMSE) values of 19.16 Mg/ha, 19.23 Mg/ha, and 25.35 Mg/ha. Predictive models for canopy cover (4.6 m) achieved R2 values of 0.93, 0.75 and 0.63 and RMSE values of 6.36%, 12.33% and 15.01% for the simulated dataset without noise, nighttime and daytime scenarios respectively.\nFindings from this study suggest the potential of ICESat-2 for estimating AGB, given the photon detection rate and background noise anticipated by ICESat-2 under temperate forest settings, and especially in the data format provided by standard ICESat-2 data vegetation products.\n","permalink":"/publications/icesat-2_biomass/","tags":["ICESat-2","AGB","Photon counting lidar","ATL08","Canopy cover","Canopy height","Carbon"],"title":"Estimating aboveground biomass and forest canopy cover with simulated ICESat-2 data"},{"categories":null,"contents":"Introduction: The assessment of forest aboveground biomass (AGB) can contribute to reducing uncertainties associated with the amount and distribution of terrestrial carbon. The Ice, Cloud and land Elevation Satellite-2 (ICESat-2) was launched on September 15th, 2018 and will provide data which will offer the possibility of assessing AGB and forest carbon at multiple spatial scales.\nGoal: The primary goal of this study was to develop an approach for utilizing data similar to ICESat-2\u0026rsquo;s land-vegetation along track product (ATL08) to generate wall-to-wall AGB maps.\nHow: Utilizing simulated daytime and nighttime ICESat-2 data from planned ICESat-2 tracks over vegetation conditions in south-east Texas, we investigated the integration of Landsat data and derived products for AGB model and map production. Linear regression models were first used to relate simulated photon-counting lidar (PCL) metrics for 100 m segments along ICESat-2 tracks to reference airborne lidar-estimated AGB over Sam Houston National Forest (SHNF) in south-east Texas. Random Forest (RF) was then used to create AGB maps from predicted AGB estimates and explanatory data consisting of spectral metrics derived from Landsat TM imagery and land cover and canopy cover data from the National Land Cover Database (NLCD).\nHighlights:\nUsing RF, AGB and AGB uncertainty maps produced at 30 m spatial resolution represented three data scenarios;\n(1) simulated ICESat-2 PCL vegetation product without the impact of noise (no noise scenario),\n(2) simulated ICESat-2 PCL vegetation product from data with noise levels associated with daytime operation of ICESat-2 (daytime scenario), and\n(3) simulated ICESat-2 PCL vegetation product from data with noise levels associated with nighttime operation of ICESat-2 (nighttime scenario).\nThe RF models exhibited moderate accuracies (0.42 to 0.51) with RMSE values between 19 Mg/ha to 20 Mg/ha with a separate test set. The adoption of a combinatory approach of simulated ICESat-2 and Landsat data could be implemented at larger spatial scales and in doing so, ancillary data such as climatic and topographic variables may be examined for improving AGB predictions.\n","permalink":"/publications/icesat-2_biomass_another/","tags":["ICESat-2","AGB mapping","Photon counting lidar","ATL08","Canopy cover","Canopy height","LandSat"],"title":"Mapping forest aboveground biomass with a simulated ICESat-2 vegetation canopy product and Landsat data"},{"categories":null,"contents":"Introduction:\nFull waveform (FW) LiDAR holds great potential for retrieving vegetation structure parameters at a high level of detail, but this prospect is constrained by practical factors such as the lack of available handy processing tools and the technical intricacy of waveform processing.\nMethods:\nThis study introduces a new product named the Hyper Point Cloud (HPC), derived from FW LiDAR data, and explores its potential applications, such as tree crown delineation using the HPC-based intensity and percentile height (PH) surfaces, which shows promise as a solution to the constraints of using FW LiDAR data.\nHighlight:\n(1) why named hyper point cloud?\nThe results of the HPC present a new direction for handling FW LiDAR data and offer prospects for studying the mid-story and understory of vegetation with high point density (~182 points/m2).\n(2) what HPC can do?\nThe intensity-derived digital surface model (DSM) generated from the HPC shows that the ground region has higher maximum intensity (MAXI) and mean intensity (MI) than the vegetation region, while having lower total intensity (TI) and number of intensities (NI) at a given grid cell.\n(3) why contour of intensity can work for tree segmentation?\nOur analysis of intensity distribution contours at the individual tree level exhibit similar patterns, indicating that the MAXI and MI decrease from the tree crown center to the tree boundary, while a rising trend is observed for TI and NI.\nThese intensity variable contours provide a theoretical justification for using HPC-based intensity surfaces to segment tree crowns and exploit their potential for extracting tree attributes. The HPC-based intensity surfaces and the HPC-based PH Canopy Height Models (CHM) demonstrate promising tree segmentation results comparable to the LiDAR-derived CHM for estimating tree attributes such as tree locations, crown widths and tree heights.\nWe envision that products such as the HPC and the HPC-based intensity and height surfaces introduced in this study can open new perspectives for the use of FW LiDAR data and alleviate the technical barrier of exploring FW LiDAR data for detailed vegetation structure characterization.\n","permalink":"/publications/hyper_point_cloud/","tags":["hyper point cloud (HPC)","HPC-based intensity surface","gridding","tree segmentation","vegetation structure"],"title":"From LiDARWaveforms to Hyper Point Clouds: A Novel Data Product to Characterize Vegetation Structure"},{"categories":null,"contents":"Introduction:\nThe upcoming Ice, Cloud and Land Elevation Satellite-2 (ICESat-2) mission will offer prospects for mapping and monitoring biomass and carbon of terrestrial ecosystems over large areas using photon counting LiDAR data\nGoal:\nWe aim to develop a methodology to derive terrain elevation and vegetation canopy height from testbed sensor data and further pre-validate the capacity of the mission to meet its science objectives for the ecosystem community.\nMethods:\nWe investigated a novel methodological framework with two essential steps for characterizing terrain and canopy height using Multiple Altimeter Beam Experimental LiDAR (MABEL) data and simulated ICESat-2 data with various vegetation conditions. Our algorithm first implements a multi-level noise filtering approach to minimize noise photons and subsequently classifies the remaining photons into ground and top of canopy using an overlapping moving window method and cubic spline interpolation.\nHighlight:\n(1) Noise filtering:\nResults of noise filtering show that the design of the multi-level filtering process is effective to identify background noise and preserve signal photons in the raw data.\n(2) Day time vs. night time\nMoreover, calibration results using MABEL and simulated ICESat-2 data share similar trends with the retrieved terrain being more accurate than the retrieved canopy height, and the nighttime results being better than corresponding daytime results.\n(3) Simulated ICESat-2 vs. MABEL\nCompared to the results of simulated ICESat-2 data, MABEL data achieve lower accuracy for ground and canopy heights in terms of root mean square error (RMSE), which may partly result from the inconsistency between MABEL and reference data. Specifically, simulated ICESat-2 data using 115 various nighttime and daytime scenarios, yield average RMSE values of 1.83m and 2.80m for estimated ground elevation, and 2.70m and 3.59m for estimated canopy height.\n(4) Percentile height validation Additionally, the accuracy assessment of percentile heights of simulated ICESat-2 data further substantiates the robustness of the methodology from different perspectives.\nThe methodology developed in this study illustrates plausible ways of processing the data that are structurally similar to expected ICESat-2 data and holds the potential to be a benchmark for further method adjustment once genuine ICESat-2 are available.\n","permalink":"/publications/photon_couting_algorithms/","tags":["ICESat-2","ATL03, ATL08","Photon classification","Photon counting LiDAR","ATLAS","MABEL","Canopy height","Terrain elevation"],"title":"Photon counting LiDAR: An adaptive ground and canopy height retrieval algorithm for ICESat-2 data"},{"categories":null,"contents":"Introduction:\nThe structural loss rates of standing dead trees (SDTs) affect a variety of processes of interest to ecologists and foresters, yet the decomposition of SDTs has been traditionally characterized by qualitative decay classes, reductions in wood density as decay progresses, and sampling schemes focused on estimating snag longevity.\nBy establishing a methodology to accurately and efficiently quantify SDT structural loss over time, these estimated structural loss rates would improve the performance of a variety of models and potentially provide new insight as to the manner in which SDTs undergo degradation in various conditions.\nGoal: The specific objective of this study were:\n  utilize the TreeVolX algorithm to estimate the volume of 29 SDTs scanned with terrestrial lidar;\n  develop a novel, voxel-based change detection algorithm capable of providing automated structural loss estimates with multitemporal terrestrial lidar observations; and\n  estimate and characterize the structural loss rates of Pinus taeda and Quercus stellata in southeastern Texas.\n  Highlights:\nA voxel-based change detection methodology was developed to accurately detect and quantify structural losses and incorporated several methods to mitigate the challenges presented by shifting tree and branch positions as SDT decay progresses.\nThe volume and structural loss of 29 SDTs, composed of Pinus taeda and Quercus stellata, were successfully estimated using multitemporal terrestrial lidar observations over elapsed times ranging from 71 to 753 days.\nPine and oak structural loss rates were characterized by estimating the amount of volumetric loss occurring in 20 equal-interval height bins of each SDT. Results showed that large pine snags exhibited more rapid structural loss in comparison to medium-sized oak snags in southeastern Texas.\n","permalink":"/publications/terrestrail_lidar_scan/","tags":["Biomass","Change detection","Reconstructed tree model (RTM)","Snag","Standing dead tree (SDT)","Structural loss","Terrestrial lidar","Volume, Voxel"],"title":"Detecting and Quantifying Standing Dead Tree Structural Loss with Reconstructed Tree Models Using Voxelized Terrestrial Lidar Data"},{"categories":null,"contents":"Introduction:\nA thorough understanding of full waveform (FW) LiDAR data processing and associated uncertainty is critical to vegetation applications such as retrieving forest structure variables and estimating forest biomass.\nGoal:\nTo qunaitfy the uncertaitny of the estimation and gain a deep understanding of waveform lidar processing.\nMethods:\nThis paper applies the Bayesian non-linear modeling concept to process small-footprint FW LiDAR data (the Bayesian decomposition) collected at a study site of the National Ecological Observatory Network (NEON) to investigate its potential for waveform decomposition and uncertainty estimation.\nSpecifically, several possible models suitable for fitting waveforms were assessed within the Bayesian framework, and the Gaussian model was selected to perform the Bayesian decomposition. Subsequently, we conducted performance evaluation and uncertainty analysis at the parameter, derived point cloud and surface model levels.\nHighlights:\n(1) which model to use?\nResults of the model reasonableness show that the Gaussian model is superior to alternative models with respect to uncertainty, physical meaning and processing efficiency.\n(2) How the Bayesiand decomposition perform?\nAfter converting waveforms to discrete points, the model comparisons demonstrate that the Bayesian decomposition can be utilized for FW LiDAR data processing, and its results are comparable to the direct decomposition (DD), Gold and RL (Richardson-Lucy) approaches in terms of the root mean squared error (RMSE \u0026lt; 0.93 m) of the point distances between the waveform-based point cloud and the reference point cloud.\n(3) which part of vegetation have evident advantage for waveform lidar?\nAdditionally, more points can be extracted from FW LiDAR data with these methods than discrete-return LiDAR data, especially at the mid-story of vegetation based on the results of height bins, percentile heights and canopy LiDAR density at the individual tree level.\n(4) Uncertainy analysis\nMoreover, uncertainty estimates from the Bayesian method enhance the credibility of decomposition results in a probabilistic sense to capture the true error of estimates and trace the uncertainty propagation along the processing steps. For example, results of the surface model yield larger RMSE values (1.38 m vs. 0.65 m) with a wider credible interval than quantile point clouds with a more compact distribution.\nadvantages of Bayesian\nIn contrast to commonly used deterministic approaches, the Bayesian decomposition method can produce an ensemble of reasonable parameter estimates with probability through Markov Chain Monte Carlo (MCMC) sampling from the posterior distribution of model parameters.\nThese parameter estimates and corresponding derived products can be queried to provide meaningful interpretation of results and associated uncertainty. Both the flat priors and empirical priors can achieve good performance of the decomposition while the empirical priors tend to significantly speed up the model convergence\nThis study provides an alternative and innovative approach for waveform processing that willbenefit high fidelity processing of waveform LiDAR data to characterize vegetation structures.\n","permalink":"/publications/bayesian_decomposition/","tags":["algorithm development","Waveform LiDAR","Bayesian inference","Tree canopy height","Model reasonableness","Decomposition","Uncertainty"],"title":"Bayesian decomposition of full waveform LiDAR data with uncertainty analysis"},{"categories":null,"contents":"Introduction:\nWaveform Light Detection and Ranging (LiDAR) data have advantages over discrete-return LiDAR data inaccurately characterizing vegetation structure. However, we lack a comprehensive understanding ofwaveform data processing approaches under different topography and vegetation conditions.\nGoal:\nThe objec-tive of this paper is to highlight a novel deconvolution algorithm, the Gold algorithm, for processingwaveform LiDAR data with optimal deconvolution parameters. Further, we present a comparative studyof waveform processing methods to provide insight into selecting an approach for a given combination ofvegetation and terrain characteristics.\nMethods:\nWe employed two waveform processing methods:\n(1) direct decomposition,\n(2) deconvolution and decomposition.\nIn method (2), we utilized two deconvolutionalgorithms - the Richardson-Lucy (RL) algorithm and the Gold algorithm.\nThe comprehensive and quan-titative comparisons were conducted in terms of the number of detected echoes, position accuracy, thebias of the end products (such as digital terrain model (DTM) and canopy height model (CHM)) fromthe corresponding reference data, along with parameter uncertainty for these end products obtainedfrom different methods.\nWhere:\nThis study was conducted at three study sites that include diverse ecological regions, vegetation and elevation gradients.\nHighlights:\n(1) Results demonstrate that two deconvolution algorithms are sensitive to the pre-processing steps of input data. The deconvolution and decomposition methodis more capable of detecting hidden echoes with a lower false echo detection rate, especially for theGold algorithm.\n(2) Compared to the reference data, all approaches generate satisfactory accuracy assess-ment results with small mean spatial difference (\u0026lt;1.22 m for DTMs, \u0026lt;0.77 m for CHMs) and root meansquare error (RMSE) (\u0026lt;1.26 m for DTMs, \u0026lt;1.93 m for CHMs). More specifically, the Gold algorithm is supe-rior to others with smaller root mean square error (RMSE) (\u0026lt;1.01 m), while the direct decompositionapproach works better in terms of the percentage of spatial difference within 0.5 and 1 m.\n(3) The parameteruncertainty analysis demonstrates that the Gold algorithm outperforms other approaches in dense veg-etation areas, with the smallest RMSE, and the RL algorithm performs better in sparse vegetation areas interms of RMSE.\n(4) Additionally, the high level of uncertainty occurs more on areas with high slope and highvegetation.\nThis study provides an alternative and innovative approach for waveform processing that willbenefit high fidelity processing of waveform LiDAR data to characterize vegetation structures.\n  ","permalink":"/publications/gold/","tags":["algorithm development","Waveform LiDAR","Deconvolution","Gold","Richardson-Lucy (RL)","Decomposition","Parameter uncertainty"],"title":"Gold-A novel deconvolution algorithm with optimization for waveform LiDAR processing"},{"categories":null,"contents":"The subsquent section is mainly to show how to quantify the uncertaity of waveform processing uinsg Bayesian method. The detailed description of the approach can refer to https://www.researchgate.net/publication/319018940_Bayesian_decomposition_of_full_waveform_LiDAR_data_with_uncertainty_analysis\nIn the domain of LiDAR applications, the observations or data are inherently subject to various errors such as system setting, system calibration, and range measurement errors. Additionally, the LiDAR vendors often do not clearly state what errors are considered when the data are provided. Thus, uncertainty of \u0026ldquo;truth\u0026rdquo; is ubiquitous and inherently present in the realities of LiDAR data modeling.\nFurthermore, the models used here are based on the non-linear functions that generally suffer from problem of non-uniqueness, which can generate different parameter combinations given the same observational data and model, or several models can fit observational data at the expense of violating the physical meaning and theoretical assumption of the \u0026ldquo;real\u0026rdquo; model.\nThese problems are more evident for the sophisticated models with multiple peak components in the waveform decomposition. Thus, estimating model uncertainty is imperative for an in-depth understanding of information derived from data and the estimation accuracy.\nOverall,the Bayesian decomposition is the intersection of waveform decompostion, Bayesian and non-linear dynamics.\nIf you find this is interesting, you can go to my github to explore the example code.\n","permalink":"/projects/creations/bayesian_decompostion/","tags":["Bayesian uncertainty","Bayesian decomposition","waveformlidar","brms","Uncertainty propragation","Gaussian","Adaptive Gaussian","Weibull"],"title":"Bayesian decompostion of waveform lidar and uncertaitny analysis"},{"categories":null,"contents":"BOSH (Bosh Outer SHell) \u0026ldquo;\u0026hellip;is an open source tool for release engineering, deployment, lifecycle management, and monitoring of distributed systems.\u0026rdquo; And it\u0026rsquo;s amazingly powerful. This examples uses BOSH to provision an Alassian vendor app running on JDK along with the support Postgres database and agents to support it. The releases manages the health of services and will automatically provision, start/stop processes across the various services.\n","permalink":"/projects/creations/tree_segmentation/","tags":["Tree segmentation","R","TreeVaw","watershed","monit","python"],"title":"Tree segmentation using free available tools"},{"categories":["R"],"contents":"\rR Markdown\rThis is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com.\nYou can embed an R code chunk like this:\nsummary(cars)\r## speed dist ## Min. : 4.0 Min. : 2.00 ## 1st Qu.:12.0 1st Qu.: 26.00 ## Median :15.0 Median : 36.00 ## Mean :15.4 Mean : 42.98 ## 3rd Qu.:19.0 3rd Qu.: 56.00 ## Max. :25.0 Max. :120.00\rfit \u0026lt;- lm(dist ~ speed, data = cars)\rfit\r## ## Call:\r## lm(formula = dist ~ speed, data = cars)\r## ## Coefficients:\r## (Intercept) speed ## -17.579 3.932\r\rIncluding Plots\rYou can also embed plots. See Figure 1 for example:\npar(mar = c(0, 1, 0, 1))\rpie(\rc(280, 60, 20),\rc(\u0026#39;Sky\u0026#39;, \u0026#39;Sunny side of pyramid\u0026#39;, \u0026#39;Shady side of pyramid\u0026#39;),\rcol = c(\u0026#39;#0292D8\u0026#39;, \u0026#39;#F7EA39\u0026#39;, \u0026#39;#C4B632\u0026#39;),\rinit.angle = -50, border = NA\r)\r\rFigure 1: A fancy pie chart.\r\r\r","permalink":"/blog/2015-07-23-r-rmarkdown/","tags":["R Markdown","plot","regression"],"title":"Hello R Markdown"},{"categories":null,"contents":"Intro Doesn\u0026rsquo;t matter whether it\u0026rsquo;s a CakePHP app for a client, your own personal CMS, or any other web based application. If your passing around passwords or other sensitive info you should really implement SSL. SSL provides 2 main perks to your visitors.\n First it encrypts all communication that flies across the web. This prevents curious or devious billies from getting your secrets. Secondly it ensures to the user that your server is in fact who it claims, and not a nasty \u0026lsquo;man in the middle\u0026rdquo; attack. Finally it gives your site that touch of class\u0026hellip;. which of course a classy person like yourself relies on.  Once you implement SSL certificates on your server you\u0026rsquo;ll want to require secure connections using Apache\u0026rsquo;s rewrite module. Now I won\u0026rsquo;t dwell on the creation and signing of certificates, its already well documented. If your just starting out though,heres a few links I recommend;\n Creating self-signed certificates (free, but should only be used internally or for testing, users will; see an \u0026lsquo;Untrusted\u0026rdquo; warning) Requesting a CA Signed certificate (not free, but the final certificate is trusted and seamless for users)  The second link uses the schools internal CA, you will need to pay a public CA like Entrust or Verisign. All of this information is aimed at \u0026lsquo;nix or solaris servers running apache. Why? cause a production windows server is laughable :-p\nNow that you have a certificate, whats next? So there you are you have a shiny new Certificate and Server key, how do you force visitors to your apache driven site to use the SSL? You copied the certificates into the appropite locations right? And you have made the needed changes in httpd.conf right? So now when you view https://example.com you see a \u0026lsquo;trusted\u0026rsquo; warning or your site right? If No to any of these than this article does a pretty good job of outlining those steps.\nThe SSL Works, How do I force connections to use it? First you need to decide if you want to force every page on your site to use SSL, or only a particular sub-domain, or maybe just your admin directory. Since the overhead is minimal there is no harm is forcing the entire domain to leverage SSL, but if it is a self-signed certificate for your personal use than you\u0026rsquo;ll most certainly want to restrict its use to your own areas. This prevents users from seeing that nasty warning \u0026ldquo;This server is not trusted\u0026rdquo; You\u0026rsquo;ll know if your using SSL because the url prefix changes from http to https (s for secure).\nForcing entire domain to use SSL You want any visit, any where to use ssl. This probably the simplest solution. Create or append to your htaccess file in the top directory of your server. Some people use a port check (80 is typically http, while 443 is https) but if you have alernate configs or the user just adds :8080 to the end of the url this method is useless. Instead check whether the https environmental variable is set, if not then redirect.\nRewriteCond %{HTTPS} !=on RewriteRule ^(.*)$ https://%{SERVER_NAME}$1 \\[R,L\\] Forcing sub-domains to use SSL Maybe you only want mysecretarea.example.com to use SSL, that\u0026rsquo;s easy enough. Its the same premise as above, but you move the htaccess file into the directory that corresponds to the subdomain. Also change the second line like below;\nRewriteCond %{HTTPS} !=on RewriteRule ^(.*)$ https://mysecretarea.%{SERVER_NAME}$1 \\[R,L\\] Forcing a directory to use SSL This method cn get a little hairier if your using aliases or redirects on top of this one. You\u0026rsquo;ll need to consider what order the commands are read. The basic principle is like so. You want all visits to example.com/admin to use ssl. Create a htaccess file in the parent directory. Again will check for the https variable, but this time we also check for the sub-directory to be in the path.\nRewriteCond %{HTTPS} !=on RewriteRule ^/admin/(.*)$ https://%{SERVER_NAME}/admin/$1 \\[R,L\\] ","permalink":"/blog/force-ssl/","tags":["apache","apache","redirect","rewrite","ssl","web development"],"title":"Forcing Visits to use SSL"},{"categories":null,"contents":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml\n[outputs] home = [\u0026#34;HTML\u0026#34;, \u0026#34;JSON\u0026#34;] Searching additional fileds To search additional fields defined in front matter, you must add it in 2 places.\nEdit layouts/_default/index.JSON This exposes the values in /index.json i.e. add category\n... \u0026#34;contents\u0026#34;:{{ .Content | plainify | jsonify }} {{ if .Params.tags }}, \u0026#34;tags\u0026#34;:{{ .Params.tags | jsonify }}{{end}}, \u0026#34;categories\u0026#34; : {{ .Params.categories | jsonify }}, ... Edit fuse.js options to Search static/js/search.js\nkeys: [ \u0026#34;title\u0026#34;, \u0026#34;contents\u0026#34;, \u0026#34;tags\u0026#34;, \u0026#34;categories\u0026#34; ] ","permalink":"/search/","tags":null,"title":"Search Results"}]