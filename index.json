[{"categories":null,"contents":"Goal:\nMake waveform lidar processing become easier and enable users can use exisitng Discrete-return lidar data processing tools.\nIntroduction:\nWaveform Light Detection and Ranging (LiDAR) data have advantages over discrete-return LiDAR data in accurately characterizing vegetation structure. However, we lack a comprehensive understanding of waveform data processing approaches under different topography and vegetation conditions. The objective of this paper is to highlight a novel deconvolution algorithm, the Gold algorithm, for processing waveform LiDAR data with optimal deconvolution parameters. Further, we present a comparative study of waveform processing methods to provide insight into selecting an approach for a given combination of vegetation and terrain characteristics. We employed two waveform processing methods: (1) direct decomposition, (2) deconvolution and decomposition. In method two, we utilized two deconvolution algorithms – the Richardson-Lucy (RL) algorithm and the Gold algorithm. The comprehensive and quantitative comparisons were conducted in terms of the number of detected echoes, position accuracy, the bias of the end products (such as digital terrain model (DTM) and canopy height model (CHM)) from the corresponding reference data, along with parameter uncertainty for these end products obtained from different methods. This study was conducted at three study sites that include diverse ecological regions, vegetation and elevation gradients. Results demonstrate that two deconvolution algorithms are sensitive to the pre-processing steps of input data. The deconvolution and decomposition method is more capable of detecting hidden echoes with a lower false echo detection rate, especially for the Gold algorithm. Compared to the reference data, all approaches generate satisfactory accuracy assessment results with small mean spatial difference (\u0026lt;1.22 m for DTMs, \u0026lt;0.77 m for CHMs) and root mean square error (RMSE) (\u0026lt;1.26 m for DTMs, \u0026lt;1.93 m for CHMs). More specifically, the Gold algorithm is superior to others with smaller root mean square error (RMSE) (\u0026lt;1.01 m), while the direct decomposition approach works better in terms of the percentage of spatial difference within 0.5 and 1 m. The parameter uncertainty analysis demonstrates that the Gold algorithm outperforms other approaches in dense vegetation areas, with the smallest RMSE, and the RL algorithm performs better in sparse vegetation areas in terms of RMSE. Additionally, the high level of uncertainty occurs more on areas with high slope and high vegetation. This study provides an alternative and innovative approach for waveform processing that will benefit high fidelity processing of waveform LiDAR data to characterize vegetation structures.\n","permalink":"/projects/creations/waveformlidar_processing/","tags":["Waveformlidar","Decompostion and deconvolution","Gaussian","Weibull","Adaptive Gaussian","Gold/RL"],"title":"waveform decomposition vs. deconvolution"},{"categories":null,"contents":"Introduction:\nEnsemble modeling is a process where multiple diverse models are created to predict an outcome, either by using many different modeling algorithms or using different training data sets. The ensemble model then aggregates the prediction of each base model and results in once final prediction for the unseen data. The motivation for using ensemble models is to reduce the generalization error of the prediction. As long as the base models are diverse and independent, the prediction error of the model decreases when the ensemble approach is used.\nThe approach seeks the wisdom of crowds in making a prediction. Even though the ensemble model has multiple base models within the model, it acts and performs as a single model.\nWhy Ensemble modeling?\nThere are two major benefits of Ensemble models:\n1 Better prediction\r2 More stable model\r The aggregate opinion of a multiple models is less noisy than other models. In finance, it was called “Diversification”, a mixed portfolio of many stocks will be much less variable than just one of the stocks alone.\nThis is also why your models will be better with ensemble of models rather than individual. One of the caution with ensemble models are over fitting although bagging takes care of it largely.\n","permalink":"/projects/creations/ensemble_models_classification/","tags":["Ensemble models","optimized NN","Machine learning","Robust to noise"],"title":"Ensemble models for classification (combine deep learning with machine learning)"},{"categories":null,"contents":"Make waveform lidar processing become easier and enable users can use exisitng Discrete-return lidar data processing tools.\nIntroduction:\nA wealth of Full Waveform (FW) LiDAR data are available to the public from different sources, which is poised to boost the extensive application of FW LiDAR data. However, we lack a handy and open source tool that can be used by potential users for processing and analyzing FW LiDAR data. To this end, we introduce waveformlidar, an R package dedicated to FW LiDAR processing, analysis and visualization as a solution to the constraint. Specifically, this package provides several commonly used waveform processing methods such as Gaussian, adaptive Gaussian and Weibull decompositions, and deconvolution approaches (Gold and Richard-Lucy (RL)) with users customized settings. In addition, we also develop some functions to derive commonly used waveform metrics for characterizing vegetation structure. Moreover, a new way to directly visualize FW LiDAR data is developed through converting waveforms into points to form the Hyper Point cloud (HPC), which can be easily adopted and subsequently analyzed with existing discrete-return LiDAR processing tools such as LAStools and FUSION. Basic explorations of the HPC such as 3D voxelization of the HPC and conversion from original waveforms to composite waveforms are also available in this package. All of these functions are developed based on small-footprint FW LiDAR data, but they can be easily transplanted to the large footprint FW LiDAR data such as Geoscience Laser Altimeter System (GLAS) and Global Ecosystem Dynamics Investigation (GEDI) data analysis.\n","permalink":"/projects/contributions/waveformlidar_r_package/","tags":["R","Waveformlidar","NEON","Decompostion and deconvolution","hyper point cloud","Gaussian","Weibull","Adaptive Gaussian","percentile height/intensity"],"title":"R package - waveformlidar data processing and analysis"},{"categories":null,"contents":"Introduction:\nGenealy, it require an amount of data to train and a long time to train a NLP model for a specific dataset. Transfer learning is commonly used in this case to conduct the sentiment analsis for Natural Language Processing (NLP) problem.\nSwivel performs approximate factorization of the point-wise mutual information matrix via stochastic gradient descent. It uses a piecewise loss with special handling for unobserved co-occurrences, and thus makes use of all the information in the matrix.\nPretrained Word Embeddings Pretrained Word Embeddings are the embeddings learned in one task that are used for solving another similar task.\nThese embeddings are trained on large datasets, saved, and then used for solving other tasks. That’s why pretrained word embeddings are a form of Transfer Learning.\nCBOW (Continuous Bag Of Words) and Skip-Gram are two most popular frames for word embedding. In CBOW the words occurring in context (surrounding words) of a selected word are used as inputs and middle or selected word as the target. Its the other way round in Skip-Gram, here the middle word tries to predict the words coming before and after it.\nWhy do we need Pretrained Word Embeddings?\nPretrained word embeddings capture the semantic and syntactic meaning of a word as they are trained on large datasets. They are capable of boosting the performance of a Natural Language Processing (NLP) model. These word embeddings come in handy during hackathons and of course, in real-world problems as well.\nBut why should we not learn our own embeddings? Well, learning word embeddings from scratch is a challenging problem due to two primary reasons:\n  1 Sparsity of training data\n  2 Large number of trainable parameters\n  ","permalink":"/projects/creations/nlp_swivel/","tags":["NLP","SWIVEL","Transfer learning","Embedding layer"],"title":"Sentiment analysis for review classification using SWIVEL and a small datasets"},{"categories":null,"contents":"Introduction Bayesian NN How much confidence do you know about your model results or a paritcular prediction?\nThis is a critical important question for many business. With the advent of deep learning, many forecasting problems for business have been solved in innovative ways. For example, Uber researchers has provided a fascianting paper on time series prediction.\nStandard deep learning method such as LSTM do not capture model uncertianty. However, the uncertianty estimation is indispensable for deep learning models.\nBayesian probability theory offers us mathematically grounded tools to reason about model uncertainty, but these usually come with a prohibitive computational cost [2].\nIn deep learning, there are two kinds of strategries to quantify the uncertianty: (1) MC dropout and (2) variational inference.\n(1) Regarding MC dropout, Gal developed a framework casting dropout training in deep neural networks (NNs) as approximate Bayesian inference in deep Gaussian processes. This method can mitigates the problem of representing model uncertainty in deep learning without sacrificing either computational complexity or test accuracy.\n(2)Variational inference such as sampling-based variational inference and stochastic variational inference has been applied to deep learning models, which have performed as well as dropout. However, this approach comes with a prohibitive computational cost. To represent uncertainty, the number of parameters in these appraoches is doubled for the same network size. Further, they require more time to converge and do not improve on existing techniques. Given that good uncertainty estimates can be cheaply obtained from common dropout models, this might result in unnecessary additional computation.\nwhat is variational inference? In short, variational inference is an approach of approximating model posterior which would otherwise be difficult to work with directly. Intuitively, this is a measure of similarity between the two distributions although it is not symmetric. So minimising this objective fits our approximating distribution to the distribution we care about.\nThis is standard in variational inference where we fit distributions rather than parameters, resulting in our robustness to over-fitting.\nHow dropout to represent Bayesian approximation Compared to standard NN, the BNN added a binary vector in each layer. We sample new realisations for the binary vectors bi for every input point and every forward pass thorough the model (evaluating the model\u0026rsquo;s output), and use the same values in the backward pass (propagating the derivatives to the parameters to be optimised W1,W2,b). The elements of vector bi take value 1 with probability 0≤pi≤1 for i=1,2\u0026hellip;l. i is the ith layer.\nThe dropped weights b1W1 and b2W2 are often scaled by 1/pi to maintain constant output magnitude. At test time we do not sample any variables and simply use the full weights matrices W1,W2,b.\nActually, the dropout network is similar to a Gaussian process approximation. Different network structures and different non-linearities would correspond to different prior beliefs as to what we expect our uncertainty to look like. This property is shared with the Gaussian process as well. Different Gaussian process covariance functions would result in different uncertainty estimates. If you are interested in more details on the BNN. Please refer to [here]（http://mlg.eng.cam.ac.uk/yarin/blog_3d801aa532c1ce.html） - Why Does It Even Make Sense?\nCalculating prediction uncertainty with BNNs developed by Uber The variance quantifies the prediction uncertainty, which can be broken down using the law of total variance. An underlying assumption for the model uncertainty equation is that yhat is generated by the same procedure, but this is not always the case. In anomaly detection, for instance, it is expected that certain time series will have patterns that differ greatly from the trained model. Therefore, reseachers from Uber propose that a complete measurement of prediction uncertainty should be composed of three parts:\n1 model uncertainty\n2 model misspecification\n3 inherent noise level.\n1 Model Uncertainty Model uncertainty, also referred to as epistemic uncertainty, captures our ignorance of the model parameters and can be reduced as more samples are collected. The key to estimating model uncertainty is the posterior distribution , also referred to as Bayesian inference. This is particularly challenging in neural networks because of the non-conjugacy often caused by nonlinearities.\nHere we used Monte Carlo dropout to approximate model uncertainty.\n2 Model misspecification Model misspecification captures the scenario where testing samples come from a different population than the training set, which is often the case in time series anomaly detection. Similar concepts have gained attention in deep learning under the concept of adversarial examples in computer vision, but its implication in prediction uncertainty remains relatively unexplored\nHere we first fit a latent embedding space for all training time series using an encoder-decoder framework. From there, we are able to measure the distance between test cases and training samples in the embedded space.\nAfter estimating uncertaitny from model misspecification, we combined model uncertianty with model misspecification uncertianty by connecting the encoder-decoder network with a prediction network, and treat them as one large network during inference.\n3 Inherent noise Inherent noise is mainly to capture the uncertainty in the data generation process and which is irreducible. Uber reseachers propose a simple but adaptive approach by estimating the noise level via the residual sum of squares, evaluated on an independent held-out validation set.\nIf you are interested in technical parts on these three sections, you can go to here for more details.\n","permalink":"/projects/creations/bayesian-uncertainty-of-neutral-networks-lstm-for-time-series-analysis/","tags":["LSTM","Bayesian Uncertainty","Autoencoder","Dropout","Model misspecification","Inherent noise"],"title":"Bayesian Uncertainty for EVI prediction using LSTM and autoencoder"},{"categories":null,"contents":"Introduction:\nThe picture of Bayesian optimization is obtianed from here\nBayesian optimization\nThere are a lot of hyperparameters for machine learning models such as NN. Typically, random or grid search are effecient ways to conduct the optimization of models. They can be very time-consuming in some cases which waste time on unpromising areas of search space. Bayesian optimization can overcome this problem by adopting an informed seach method in the space to find the optmized parameters.\nBayesian optimization works by constructing a posterior distribution of functions (gaussian process) that best describes the function you want to optimize. As the number of observations grows, the posterior distribution improves, and the algorithm becomes more certain of which regions in parameter space are worth exploring and which are not. You can find more information and explination here\nIt\u0026rsquo;s worthy to note that the Bayesian optimization is to find the maximum of a function. Thus, when we formulate a function and evaulation metrics, we should take this part into consideration. For example, when we used log loss to evaluate our model performance, the smaller values will be better. We should return a negative logloss to make it suitable for maximum of the defined function.\nNote: It took a long time to run if you have a big dataset and wide boundary. You can refer to Colab for running the code.\n","permalink":"/projects/creations/bayesian_optimziation_nn/","tags":["Neutral Networks","Bayesian optimization","Classification","Architecture Optimization"],"title":"Bayesian optimization deep learning"},{"categories":null,"contents":"Make waveform lidar processing become easier and enable users can use exisitng Discrete-return lidar data processing tools.\nIntroduction:\nA wealth of Full Waveform (FW) LiDAR data are available to the public from different sources, which is poised to boost the extensive application of FW LiDAR data. However, we lack a handy and open source tool that can be used by potential users for processing and analyzing FW LiDAR data. To this end, we introduce waveformlidar, an R package dedicated to FW LiDAR processing, analysis and visualization as a solution to the constraint. Specifically, this package provides several commonly used waveform processing methods such as Gaussian, adaptive Gaussian and Weibull decompositions, and deconvolution approaches (Gold and Richard-Lucy (RL)) with users customized settings. In addition, we also develop some functions to derive commonly used waveform metrics for characterizing vegetation structure. Moreover, a new way to directly visualize FW LiDAR data is developed through converting waveforms into points to form the Hyper Point cloud (HPC), which can be easily adopted and subsequently analyzed with existing discrete-return LiDAR processing tools such as LAStools and FUSION. Basic explorations of the HPC such as 3D voxelization of the HPC and conversion from original waveforms to composite waveforms are also available in this package. All of these functions are developed based on small-footprint FW LiDAR data, but they can be easily transplanted to the large footprint FW LiDAR data such as Geoscience Laser Altimeter System (GLAS) and Global Ecosystem Dynamics Investigation (GEDI) data analysis.\n","permalink":"/projects/creations/waveformlidar_r_package/","tags":["R","Waveformlidar","NEON","Decompostion and deconvolution","hyper point cloud","Gaussian","Weibull","Adaptive Gaussian","percentile height/intensity"],"title":"R package - waveformlidar data processing and analysis"},{"categories":null,"contents":"Introduction:\nSmall unmanned aerial systems (UAS) have emerged as high-throughput platforms for the collection of high-resolution image data over large crop fields to support precision agriculture and plant breeding research. At the same time, the improved efficiency in image capture is leading to massive datasets, which pose analysis challenges in providing needed phenotypic data.\nTo complement these high-throughput platforms, there is an increasing need in crop improvement to develop robust image analysis methods to analyze large amount of image data. Analysis approaches based on deep learning models are currently the most promising and show unparalleled performance in analyzing large image datasets.\nGoal:\nThis study developed and applied an image analysis approach based on a SegNet deep learning semantic segmentation model to estimate sorghum panicles counts, which are critical phenotypic data in sorghum crop improvement, from UAS images over selected sorghum experimental plots.\nMethods:\nThe SegNet model was trained to semantically segment UAS images into sorghum panicles, foliage and the exposed ground using 462, 250 * 250 labeled images, which was then applied to field orthomosaic to generate a field-level semantic segmentation.\nHighlights:\n(1) post processing\nIndividual panicle locations were obtained after post-processing the segmentation output to remove small objects and split merged panicles. A comparison between model panicle count estimates and manually digitized panicle locations in 60 randomly selected plots showed an overall detection accuracy of 94%.\n(2) plot level\nA per-plot panicle count comparison also showed high agreement between estimated and reference panicle counts (Spearman correlation r = 0.88, mean bias = 0.65).\n(3) where is the error coming from?\nMisclassifications of panicles during the semantic segmentation step and mosaicking errors in the field orthomosaic contributed mainly to panicle detection errors.\nOverall, the approach based on deep learning semantic segmentation showed good promise and with a larger labeled dataset and extensive hyper-parameter tuning, should provide even more robust and effective characterization of sorghum panicle counts.\n","permalink":"/publications/deep_learning_count_panicles/","tags":["deep learning","semantic segmentation","sorghum panicle","plant phenotyping","unmanned aerial systems (UAS)","plant breeding","automation","counting"],"title":"A Deep Learning Semantic Segmentation-Based Approach for Field-Level Sorghum Panicle Counting"},{"categories":null,"contents":"Introduction:\nA plethora of information contained in full-waveform (FW) Light Detection and Ranging (LiDAR) data offers prospects for characterizing vegetation structures.\nGoal:\nThis study aims to investigate the capacity of FW LiDAR data alone for tree species identification through the integration of waveform metrics with machine learning methods and Bayesian inference.\nMethods:\nSpecifically, we first conducted automatic tree segmentation based on the waveform-based canopy height model (CHM) using three approaches including TreeVaW, watershed algorithms and the combination of TreeVaW and watershed (TW) algorithms.\nSubsequently, the Random forests (RF) and Conditional inference forests (CF) models were employed to identify important tree-level waveform metrics derived from three distinct sources, such as raw waveforms, composite waveforms, the waveform-based point cloud and the combined variables from these three sources.\nFurther, we discriminated tree (gray pine, blue oak, interior live oak) and shrub species through the RF, CF and Bayesian multinomial logistic regression (BMLR) using important waveform metrics identified in this study.\nHighlights:\n(1) tree segmentation:\nResults of the tree segmentation demonstrated that the TW algorithms outperformed other algorithms for delineating individual tree crowns.\n(2) RF vs. CF\nThe CF model overcomes waveform metrics selection bias caused by the RF model which favors correlated metrics and enhances the accuracy of subsequent classification.\n(3) Composite waveform vs. raw waveform\nWe also found that composite waveforms are more informative than raw waveforms and waveform-based point cloud for characterizing tree species in our study area.\n(4) RF vs. Bayesian\nBoth classicalmachine learning methods (the RF and CF) and the BMLR generated satisfactory average overall accuracy (74% for the RF, 77% for the CF and 81% for the BMLR) and the BMLR slightly outperformed the other two methods.\nHowever, these three methods suffered from low individual classification accuracy for the blue oak which is prone to being misclassified as the interior live oak due to the similar characteristics of blue oak and interior live oak.\nUncertainty estimates from the BMLR method compensate for this downside by providing classification results in a probabilistic sense and rendering users with more confidence in interpreting and applying classification results to real-world tasks such as forest inventory.\n(5) feature selection:\nOverall, this study recommends the CF method for feature selection and suggests that BMLR could be a superior alternative to classical machining learning methods.\n","permalink":"/publications/bayesian_machine_learning_new/","tags":["Bayesian multinomial logistic regression","Conditional inference forests","Machine learning, Random forests","Waveform signatures","Tree segmentation","Watershed","composite waveform"],"title":"Bayesian and Classical Machine Learning Methods: A Comparison for Tree Species Classification with LiDAR Waveform Signatures"},{"categories":null,"contents":"Introduction:\nA wealth of FullWaveform (FW) LiDAR (Light Detection and Ranging) data are available to the public from different sources, which is poised to boost extensive applications of FW LiDAR data. However, we lack a handy and open source tool that can be used by potential users for processing and analyzing FW LiDAR data.\nGoal: To this end, we introduce waveformlidar, an R package dedicated to FW LiDAR processing, analysis and visualization as a solution to the constraint.\nWhat this package provide:\n(1) This package provides several commonly used waveform processing methods such as Gaussian, Adaptive Gaussian and Weibull decompositions and deconvolution approaches (Gold and Richard-Lucy (RL)) with users\u0026rsquo; customized settings.\n(2) we also developed functions to derive commonly used waveform metrics for characterizing vegetation structure.\n(3) Moreover, a new way to directly visualize FW LiDAR data is developed by converting waveforms into points to form the Hyper Point Cloud (HPC), which can be easily adopted and subsequently analyzed with existing discrete-return LiDAR processing tools such as LAStools and FUSION.\n(4) Basic explorations of the HPC such as 3D voxelization of the HPC and conversion from original waveforms to composite waveforms are also available in this package.\nAll of these functions are developed based on small-footprint FW LiDAR data but they can be easily transplanted to the large footprint FW LiDAR data such as Geoscience Laser Altimeter System (GLAS) and Global Ecosystem Dynamics Investigation (GEDI) data analysis.\nIt is anticipated that these functions will facilitate the widespread use of FW LiDAR and be beneficial for better estimating biomass and characterizing vegetation structure at various scales.\n","permalink":"/publications/waveformlidar_r_package/","tags":["waveform decomposition","hyper point cloud","deconvolution","Waveform signatures","waveform gridding \u0026 voxel","composite waveform"],"title":"waveformlidar: An R Package for Waveform LiDAR Processing and Analysis"},{"categories":null,"contents":"Introduction:\nThe Ice, Cloud and Land Elevation Satellite-2 (ICESat-2) launched on September 15th, 2018 and this mission offers an extraordinary opportunity to contribute to an assessment of forest resources at multiple spatial scales. This study served to develop a methodology for utilizing ICESat-2 data over vegetated areas.\nGoal: The specific objectives were to: (1) derive a simulated ICESat-2 photon-counting lidar (PCL) vegetation product using airborne lidar data,\n(2) examine the use of simulated PCL metrics for modelling aboveground biomass (AGB) along ICESat-2 profiles using a simulated ICESat-2 PCL vegetation product and reference AGB estimated from airborne lidar data, and\n(3) estimate forest canopy cover using simulated PCL canopy product data and airborne lidarderived canopy cover.\nWhere and scenarios: Using existing airborne lidar data for Sam Houston National Forest (SHNF) in Texas and known ICESat-2 track locations, PCL simulations were carried out. Three scenarios were analyzed in this study;\n  simulated data without the addition of noise,\n  processed simulated data for daytime, and\n  nighttime scenarios.\n  Highlights:\nproduct Segments measuring 100m along the proposed ICESat-2 tracks were used to extract simulated PCL metrics from each of the three data scenarios and spatially coincident, reference airborne lidar-estimated AGB and airborne lidar canopy cover estimates.\nLinear regression models were then developed with a subset of the simulated PCL segments to estimate AGB and canopy cover and their performance assessed using separate testing sets.\nThree scenarios: AGB model testing with the simulated dataset without noise, nighttime and daytime scenarios resulted in R2 values of 0.79, 0.79 and 0.63 respectively, with root mean square error (RMSE) values of 19.16 Mg/ha, 19.23 Mg/ha, and 25.35 Mg/ha. Predictive models for canopy cover (4.6 m) achieved R2 values of 0.93, 0.75 and 0.63 and RMSE values of 6.36%, 12.33% and 15.01% for the simulated dataset without noise, nighttime and daytime scenarios respectively.\nFindings from this study suggest the potential of ICESat-2 for estimating AGB, given the photon detection rate and background noise anticipated by ICESat-2 under temperate forest settings, and especially in the data format provided by standard ICESat-2 data vegetation products.\n","permalink":"/publications/icesat-2_biomass/","tags":["ICESat-2","AGB","Photon counting lidar","ATL08","Canopy cover","Canopy height","Carbon"],"title":"Estimating aboveground biomass and forest canopy cover with simulated ICESat-2 data"},{"categories":null,"contents":"Introduction: The assessment of forest aboveground biomass (AGB) can contribute to reducing uncertainties associated with the amount and distribution of terrestrial carbon. The Ice, Cloud and land Elevation Satellite-2 (ICESat-2) was launched on September 15th, 2018 and will provide data which will offer the possibility of assessing AGB and forest carbon at multiple spatial scales.\nGoal: The primary goal of this study was to develop an approach for utilizing data similar to ICESat-2\u0026rsquo;s land-vegetation along track product (ATL08) to generate wall-to-wall AGB maps.\nHow: Utilizing simulated daytime and nighttime ICESat-2 data from planned ICESat-2 tracks over vegetation conditions in south-east Texas, we investigated the integration of Landsat data and derived products for AGB model and map production. Linear regression models were first used to relate simulated photon-counting lidar (PCL) metrics for 100 m segments along ICESat-2 tracks to reference airborne lidar-estimated AGB over Sam Houston National Forest (SHNF) in south-east Texas. Random Forest (RF) was then used to create AGB maps from predicted AGB estimates and explanatory data consisting of spectral metrics derived from Landsat TM imagery and land cover and canopy cover data from the National Land Cover Database (NLCD).\nHighlights:\nUsing RF, AGB and AGB uncertainty maps produced at 30 m spatial resolution represented three data scenarios;\n(1) simulated ICESat-2 PCL vegetation product without the impact of noise (no noise scenario),\n(2) simulated ICESat-2 PCL vegetation product from data with noise levels associated with daytime operation of ICESat-2 (daytime scenario), and\n(3) simulated ICESat-2 PCL vegetation product from data with noise levels associated with nighttime operation of ICESat-2 (nighttime scenario).\nThe RF models exhibited moderate accuracies (0.42 to 0.51) with RMSE values between 19 Mg/ha to 20 Mg/ha with a separate test set. The adoption of a combinatory approach of simulated ICESat-2 and Landsat data could be implemented at larger spatial scales and in doing so, ancillary data such as climatic and topographic variables may be examined for improving AGB predictions.\n","permalink":"/publications/icesat-2_biomass_another/","tags":["ICESat-2","AGB mapping","Photon counting lidar","ATL08","Canopy cover","Canopy height","LandSat"],"title":"Mapping forest aboveground biomass with a simulated ICESat-2 vegetation canopy product and Landsat data"},{"categories":null,"contents":"Introduction:\nFull waveform (FW) LiDAR holds great potential for retrieving vegetation structure parameters at a high level of detail, but this prospect is constrained by practical factors such as the lack of available handy processing tools and the technical intricacy of waveform processing.\nMethods:\nThis study introduces a new product named the Hyper Point Cloud (HPC), derived from FW LiDAR data, and explores its potential applications, such as tree crown delineation using the HPC-based intensity and percentile height (PH) surfaces, which shows promise as a solution to the constraints of using FW LiDAR data.\nHighlight:\n(1) why named hyper point cloud?\nThe results of the HPC present a new direction for handling FW LiDAR data and offer prospects for studying the mid-story and understory of vegetation with high point density (~182 points/m2).\n(2) what HPC can do?\nThe intensity-derived digital surface model (DSM) generated from the HPC shows that the ground region has higher maximum intensity (MAXI) and mean intensity (MI) than the vegetation region, while having lower total intensity (TI) and number of intensities (NI) at a given grid cell.\n(3) why contour of intensity can work for tree segmentation?\nOur analysis of intensity distribution contours at the individual tree level exhibit similar patterns, indicating that the MAXI and MI decrease from the tree crown center to the tree boundary, while a rising trend is observed for TI and NI.\nThese intensity variable contours provide a theoretical justification for using HPC-based intensity surfaces to segment tree crowns and exploit their potential for extracting tree attributes. The HPC-based intensity surfaces and the HPC-based PH Canopy Height Models (CHM) demonstrate promising tree segmentation results comparable to the LiDAR-derived CHM for estimating tree attributes such as tree locations, crown widths and tree heights.\nWe envision that products such as the HPC and the HPC-based intensity and height surfaces introduced in this study can open new perspectives for the use of FW LiDAR data and alleviate the technical barrier of exploring FW LiDAR data for detailed vegetation structure characterization.\n","permalink":"/publications/hyper_point_cloud/","tags":["hyper point cloud (HPC)","HPC-based intensity surface","gridding","tree segmentation","vegetation structure"],"title":"From LiDARWaveforms to Hyper Point Clouds: A Novel Data Product to Characterize Vegetation Structure"},{"categories":null,"contents":"Introduction:\nThe upcoming Ice, Cloud and Land Elevation Satellite-2 (ICESat-2) mission will offer prospects for mapping and monitoring biomass and carbon of terrestrial ecosystems over large areas using photon counting LiDAR data\nGoal:\nWe aim to develop a methodology to derive terrain elevation and vegetation canopy height from testbed sensor data and further pre-validate the capacity of the mission to meet its science objectives for the ecosystem community.\nMethods:\nWe investigated a novel methodological framework with two essential steps for characterizing terrain and canopy height using Multiple Altimeter Beam Experimental LiDAR (MABEL) data and simulated ICESat-2 data with various vegetation conditions. Our algorithm first implements a multi-level noise filtering approach to minimize noise photons and subsequently classifies the remaining photons into ground and top of canopy using an overlapping moving window method and cubic spline interpolation.\nHighlight:\n(1) Noise filtering:\nResults of noise filtering show that the design of the multi-level filtering process is effective to identify background noise and preserve signal photons in the raw data.\n(2) Day time vs. night time\nMoreover, calibration results using MABEL and simulated ICESat-2 data share similar trends with the retrieved terrain being more accurate than the retrieved canopy height, and the nighttime results being better than corresponding daytime results.\n(3) Simulated ICESat-2 vs. MABEL\nCompared to the results of simulated ICESat-2 data, MABEL data achieve lower accuracy for ground and canopy heights in terms of root mean square error (RMSE), which may partly result from the inconsistency between MABEL and reference data. Specifically, simulated ICESat-2 data using 115 various nighttime and daytime scenarios, yield average RMSE values of 1.83m and 2.80m for estimated ground elevation, and 2.70m and 3.59m for estimated canopy height.\n(4) Percentile height validation Additionally, the accuracy assessment of percentile heights of simulated ICESat-2 data further substantiates the robustness of the methodology from different perspectives.\nThe methodology developed in this study illustrates plausible ways of processing the data that are structurally similar to expected ICESat-2 data and holds the potential to be a benchmark for further method adjustment once genuine ICESat-2 are available.\n","permalink":"/publications/photon_couting_algorithms/","tags":["ICESat-2","ATL03, ATL08","Photon classification","Photon counting LiDAR","ATLAS","MABEL","Canopy height","Terrain elevation"],"title":"Photon counting LiDAR: An adaptive ground and canopy height retrieval algorithm for ICESat-2 data"},{"categories":null,"contents":"Introduction:\nThe structural loss rates of standing dead trees (SDTs) affect a variety of processes of interest to ecologists and foresters, yet the decomposition of SDTs has been traditionally characterized by qualitative decay classes, reductions in wood density as decay progresses, and sampling schemes focused on estimating snag longevity.\nBy establishing a methodology to accurately and efficiently quantify SDT structural loss over time, these estimated structural loss rates would improve the performance of a variety of models and potentially provide new insight as to the manner in which SDTs undergo degradation in various conditions.\nGoal: The specific objective of this study were:\n  utilize the TreeVolX algorithm to estimate the volume of 29 SDTs scanned with terrestrial lidar;\n  develop a novel, voxel-based change detection algorithm capable of providing automated structural loss estimates with multitemporal terrestrial lidar observations; and\n  estimate and characterize the structural loss rates of Pinus taeda and Quercus stellata in southeastern Texas.\n  Highlights:\nA voxel-based change detection methodology was developed to accurately detect and quantify structural losses and incorporated several methods to mitigate the challenges presented by shifting tree and branch positions as SDT decay progresses.\nThe volume and structural loss of 29 SDTs, composed of Pinus taeda and Quercus stellata, were successfully estimated using multitemporal terrestrial lidar observations over elapsed times ranging from 71 to 753 days.\nPine and oak structural loss rates were characterized by estimating the amount of volumetric loss occurring in 20 equal-interval height bins of each SDT. Results showed that large pine snags exhibited more rapid structural loss in comparison to medium-sized oak snags in southeastern Texas.\n","permalink":"/publications/terrestrail_lidar_scan/","tags":["Biomass","Change detection","Reconstructed tree model (RTM)","Snag","Standing dead tree (SDT)","Structural loss","Terrestrial lidar","Volume, Voxel"],"title":"Detecting and Quantifying Standing Dead Tree Structural Loss with Reconstructed Tree Models Using Voxelized Terrestrial Lidar Data"},{"categories":null,"contents":"Introduction:\nA thorough understanding of full waveform (FW) LiDAR data processing and associated uncertainty is critical to vegetation applications such as retrieving forest structure variables and estimating forest biomass.\nGoal:\nTo qunaitfy the uncertaitny of the estimation and gain a deep understanding of waveform lidar processing.\nMethods:\nThis paper applies the Bayesian non-linear modeling concept to process small-footprint FW LiDAR data (the Bayesian decomposition) collected at a study site of the National Ecological Observatory Network (NEON) to investigate its potential for waveform decomposition and uncertainty estimation.\nSpecifically, several possible models suitable for fitting waveforms were assessed within the Bayesian framework, and the Gaussian model was selected to perform the Bayesian decomposition. Subsequently, we conducted performance evaluation and uncertainty analysis at the parameter, derived point cloud and surface model levels.\nHighlights:\n(1) which model to use?\nResults of the model reasonableness show that the Gaussian model is superior to alternative models with respect to uncertainty, physical meaning and processing efficiency.\n(2) How the Bayesiand decomposition perform?\nAfter converting waveforms to discrete points, the model comparisons demonstrate that the Bayesian decomposition can be utilized for FW LiDAR data processing, and its results are comparable to the direct decomposition (DD), Gold and RL (Richardson-Lucy) approaches in terms of the root mean squared error (RMSE \u0026lt; 0.93 m) of the point distances between the waveform-based point cloud and the reference point cloud.\n(3) which part of vegetation have evident advantage for waveform lidar?\nAdditionally, more points can be extracted from FW LiDAR data with these methods than discrete-return LiDAR data, especially at the mid-story of vegetation based on the results of height bins, percentile heights and canopy LiDAR density at the individual tree level.\n(4) Uncertainy analysis\nMoreover, uncertainty estimates from the Bayesian method enhance the credibility of decomposition results in a probabilistic sense to capture the true error of estimates and trace the uncertainty propagation along the processing steps. For example, results of the surface model yield larger RMSE values (1.38 m vs. 0.65 m) with a wider credible interval than quantile point clouds with a more compact distribution.\nadvantages of Bayesian\nIn contrast to commonly used deterministic approaches, the Bayesian decomposition method can produce an ensemble of reasonable parameter estimates with probability through Markov Chain Monte Carlo (MCMC) sampling from the posterior distribution of model parameters.\nThese parameter estimates and corresponding derived products can be queried to provide meaningful interpretation of results and associated uncertainty. Both the flat priors and empirical priors can achieve good performance of the decomposition while the empirical priors tend to significantly speed up the model convergence\nThis study provides an alternative and innovative approach for waveform processing that willbenefit high fidelity processing of waveform LiDAR data to characterize vegetation structures.\n","permalink":"/publications/bayesian_decomposition/","tags":["algorithm development","Waveform LiDAR","Bayesian inference","Tree canopy height","Model reasonableness","Decomposition","Uncertainty"],"title":"Bayesian decomposition of full waveform LiDAR data with uncertainty analysis"},{"categories":null,"contents":"Introduction:\nWaveform Light Detection and Ranging (LiDAR) data have advantages over discrete-return LiDAR data inaccurately characterizing vegetation structure. However, we lack a comprehensive understanding ofwaveform data processing approaches under different topography and vegetation conditions.\nGoal:\nThe objec-tive of this paper is to highlight a novel deconvolution algorithm, the Gold algorithm, for processingwaveform LiDAR data with optimal deconvolution parameters. Further, we present a comparative studyof waveform processing methods to provide insight into selecting an approach for a given combination ofvegetation and terrain characteristics.\nMethods:\nWe employed two waveform processing methods:\n(1) direct decomposition,\n(2) deconvolution and decomposition.\nIn method (2), we utilized two deconvolutionalgorithms - the Richardson-Lucy (RL) algorithm and the Gold algorithm.\nThe comprehensive and quan-titative comparisons were conducted in terms of the number of detected echoes, position accuracy, thebias of the end products (such as digital terrain model (DTM) and canopy height model (CHM)) fromthe corresponding reference data, along with parameter uncertainty for these end products obtainedfrom different methods.\nWhere:\nThis study was conducted at three study sites that include diverse ecological regions, vegetation and elevation gradients.\nHighlights:\n(1) Results demonstrate that two deconvolution algorithms are sensitive to the pre-processing steps of input data. The deconvolution and decomposition methodis more capable of detecting hidden echoes with a lower false echo detection rate, especially for theGold algorithm.\n(2) Compared to the reference data, all approaches generate satisfactory accuracy assess-ment results with small mean spatial difference (\u0026lt;1.22 m for DTMs, \u0026lt;0.77 m for CHMs) and root meansquare error (RMSE) (\u0026lt;1.26 m for DTMs, \u0026lt;1.93 m for CHMs). More specifically, the Gold algorithm is supe-rior to others with smaller root mean square error (RMSE) (\u0026lt;1.01 m), while the direct decompositionapproach works better in terms of the percentage of spatial difference within 0.5 and 1 m.\n(3) The parameteruncertainty analysis demonstrates that the Gold algorithm outperforms other approaches in dense veg-etation areas, with the smallest RMSE, and the RL algorithm performs better in sparse vegetation areas interms of RMSE.\n(4) Additionally, the high level of uncertainty occurs more on areas with high slope and highvegetation.\nThis study provides an alternative and innovative approach for waveform processing that willbenefit high fidelity processing of waveform LiDAR data to characterize vegetation structures.\n  ","permalink":"/publications/gold/","tags":["algorithm development","Waveform LiDAR","Deconvolution","Gold","Richardson-Lucy (RL)","Decomposition","Parameter uncertainty"],"title":"Gold-A novel deconvolution algorithm with optimization for waveform LiDAR processing"},{"categories":null,"contents":"The subsquent section is mainly to show how to quantify the uncertaity of waveform processing uinsg Bayesian method. The detailed description of the approach can refer to https://www.researchgate.net/publication/319018940_Bayesian_decomposition_of_full_waveform_LiDAR_data_with_uncertainty_analysis\nIn the domain of LiDAR applications, the observations or data are inherently subject to various errors such as system setting, system calibration, and range measurement errors. Additionally, the LiDAR vendors often do not clearly state what errors are considered when the data are provided. Thus, uncertainty of \u0026ldquo;truth\u0026rdquo; is ubiquitous and inherently present in the realities of LiDAR data modeling.\nFurthermore, the models used here are based on the non-linear functions that generally suffer from problem of non-uniqueness, which can generate different parameter combinations given the same observational data and model, or several models can fit observational data at the expense of violating the physical meaning and theoretical assumption of the \u0026ldquo;real\u0026rdquo; model.\nThese problems are more evident for the sophisticated models with multiple peak components in the waveform decomposition. Thus, estimating model uncertainty is imperative for an in-depth understanding of information derived from data and the estimation accuracy.\nOverall,the Bayesian decomposition is the intersection of waveform decompostion, Bayesian and non-linear dynamics.\nIf you find this is interesting, you can go to my github to explore the example code.\n","permalink":"/projects/creations/bayesian_decompostion/","tags":["Uncertainty","Bayesian","waveformlidar","brms","Uncertainty propragation"],"title":"Bayesian decompostion of waveform lidar and uncertaitny analysis"},{"categories":null,"contents":"BOSH (Bosh Outer SHell) \u0026ldquo;\u0026hellip;is an open source tool for release engineering, deployment, lifecycle management, and monitoring of distributed systems.\u0026rdquo; And it\u0026rsquo;s amazingly powerful. This examples uses BOSH to provision an Alassian vendor app running on JDK along with the support Postgres database and agents to support it. The releases manages the health of services and will automatically provision, start/stop processes across the various services.\n","permalink":"/projects/creations/tree_segmentation/","tags":["Tree segmentation","R","TreeVaw","watershed","monit","python"],"title":"Tree segmentation using free available tools"},{"categories":["R"],"contents":"\rR Markdown\rThis is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com.\nYou can embed an R code chunk like this:\nsummary(cars)\r## speed dist ## Min. : 4.0 Min. : 2.00 ## 1st Qu.:12.0 1st Qu.: 26.00 ## Median :15.0 Median : 36.00 ## Mean :15.4 Mean : 42.98 ## 3rd Qu.:19.0 3rd Qu.: 56.00 ## Max. :25.0 Max. :120.00\rfit \u0026lt;- lm(dist ~ speed, data = cars)\rfit\r## ## Call:\r## lm(formula = dist ~ speed, data = cars)\r## ## Coefficients:\r## (Intercept) speed ## -17.579 3.932\r\rIncluding Plots\rYou can also embed plots. See Figure 1 for example:\npar(mar = c(0, 1, 0, 1))\rpie(\rc(280, 60, 20),\rc(\u0026#39;Sky\u0026#39;, \u0026#39;Sunny side of pyramid\u0026#39;, \u0026#39;Shady side of pyramid\u0026#39;),\rcol = c(\u0026#39;#0292D8\u0026#39;, \u0026#39;#F7EA39\u0026#39;, \u0026#39;#C4B632\u0026#39;),\rinit.angle = -50, border = NA\r)\r\rFigure 1: A fancy pie chart.\r\r\r","permalink":"/blog/2015-07-23-r-rmarkdown/","tags":["R Markdown","plot","regression"],"title":"Hello R Markdown"},{"categories":null,"contents":"Intro Doesn\u0026rsquo;t matter whether it\u0026rsquo;s a CakePHP app for a client, your own personal CMS, or any other web based application. If your passing around passwords or other sensitive info you should really implement SSL. SSL provides 2 main perks to your visitors.\n First it encrypts all communication that flies across the web. This prevents curious or devious billies from getting your secrets. Secondly it ensures to the user that your server is in fact who it claims, and not a nasty \u0026lsquo;man in the middle\u0026rdquo; attack. Finally it gives your site that touch of class\u0026hellip;. which of course a classy person like yourself relies on.  Once you implement SSL certificates on your server you\u0026rsquo;ll want to require secure connections using Apache\u0026rsquo;s rewrite module. Now I won\u0026rsquo;t dwell on the creation and signing of certificates, its already well documented. If your just starting out though,heres a few links I recommend;\n Creating self-signed certificates (free, but should only be used internally or for testing, users will; see an \u0026lsquo;Untrusted\u0026rdquo; warning) Requesting a CA Signed certificate (not free, but the final certificate is trusted and seamless for users)  The second link uses the schools internal CA, you will need to pay a public CA like Entrust or Verisign. All of this information is aimed at \u0026lsquo;nix or solaris servers running apache. Why? cause a production windows server is laughable :-p\nNow that you have a certificate, whats next? So there you are you have a shiny new Certificate and Server key, how do you force visitors to your apache driven site to use the SSL? You copied the certificates into the appropite locations right? And you have made the needed changes in httpd.conf right? So now when you view https://example.com you see a \u0026lsquo;trusted\u0026rsquo; warning or your site right? If No to any of these than this article does a pretty good job of outlining those steps.\nThe SSL Works, How do I force connections to use it? First you need to decide if you want to force every page on your site to use SSL, or only a particular sub-domain, or maybe just your admin directory. Since the overhead is minimal there is no harm is forcing the entire domain to leverage SSL, but if it is a self-signed certificate for your personal use than you\u0026rsquo;ll most certainly want to restrict its use to your own areas. This prevents users from seeing that nasty warning \u0026ldquo;This server is not trusted\u0026rdquo; You\u0026rsquo;ll know if your using SSL because the url prefix changes from http to https (s for secure).\nForcing entire domain to use SSL You want any visit, any where to use ssl. This probably the simplest solution. Create or append to your htaccess file in the top directory of your server. Some people use a port check (80 is typically http, while 443 is https) but if you have alernate configs or the user just adds :8080 to the end of the url this method is useless. Instead check whether the https environmental variable is set, if not then redirect.\nRewriteCond %{HTTPS} !=on RewriteRule ^(.*)$ https://%{SERVER_NAME}$1 \\[R,L\\] Forcing sub-domains to use SSL Maybe you only want mysecretarea.example.com to use SSL, that\u0026rsquo;s easy enough. Its the same premise as above, but you move the htaccess file into the directory that corresponds to the subdomain. Also change the second line like below;\nRewriteCond %{HTTPS} !=on RewriteRule ^(.*)$ https://mysecretarea.%{SERVER_NAME}$1 \\[R,L\\] Forcing a directory to use SSL This method cn get a little hairier if your using aliases or redirects on top of this one. You\u0026rsquo;ll need to consider what order the commands are read. The basic principle is like so. You want all visits to example.com/admin to use ssl. Create a htaccess file in the parent directory. Again will check for the https variable, but this time we also check for the sub-directory to be in the path.\nRewriteCond %{HTTPS} !=on RewriteRule ^/admin/(.*)$ https://%{SERVER_NAME}/admin/$1 \\[R,L\\] ","permalink":"/blog/force-ssl/","tags":["apache","apache","redirect","rewrite","ssl","web development"],"title":"Forcing Visits to use SSL"},{"categories":null,"contents":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml\n[outputs] home = [\u0026#34;HTML\u0026#34;, \u0026#34;JSON\u0026#34;] Searching additional fileds To search additional fields defined in front matter, you must add it in 2 places.\nEdit layouts/_default/index.JSON This exposes the values in /index.json i.e. add category\n... \u0026#34;contents\u0026#34;:{{ .Content | plainify | jsonify }} {{ if .Params.tags }}, \u0026#34;tags\u0026#34;:{{ .Params.tags | jsonify }}{{end}}, \u0026#34;categories\u0026#34; : {{ .Params.categories | jsonify }}, ... Edit fuse.js options to Search static/js/search.js\nkeys: [ \u0026#34;title\u0026#34;, \u0026#34;contents\u0026#34;, \u0026#34;tags\u0026#34;, \u0026#34;categories\u0026#34; ] ","permalink":"/search/","tags":null,"title":"Search Results"}]